{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "\n",
    "#Number of grid points along x and y axis\n",
    "Nx = 101\n",
    "Ny = 101\n",
    "\n",
    "#domain size\n",
    "xmin = 0.0\n",
    "xmax = 1.0\n",
    "ymin = 0.0\n",
    "ymax = 1.0\n",
    "\n",
    "#grid size\n",
    "hx = (xmax - xmin)/(Nx - 1)\n",
    "hy = (ymax - ymin)/(Ny - 1)\n",
    "\n",
    "#weight \n",
    "omega = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function considers all the inner points and organize the nodal values for the calculation of e\n",
    "#input_shape = (Nx, Ny)\n",
    "#output_shape = (number of edges, 2)\n",
    "def organize_interaction(A):\n",
    "\n",
    "    first = tf.concat([tf.reshape(A[1:, 1:-1], (-1, 1)), tf.reshape(A[0:-1, 1:-1], (-1, 1))], axis=1)\n",
    "    \n",
    "    second = tf.concat([tf.reshape(A[1:-1, 1:], (-1, 1)), tf.reshape(A[1:-1, 0:-1], (-1, 1))], axis=1)\n",
    "\n",
    "    input = tf.concat([first, second], axis=0)\n",
    "    \n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multiplying the result of f(R) with PM matrix gives c_{i,j}\n",
    "def calc_PM(Nx, Ny):\n",
    "    #number of division along x and y axes\n",
    "    m = Nx - 1\n",
    "    n = Ny - 1\n",
    "\n",
    "    num_interactions = (n-1)*m + (m-1)*n\n",
    "    num_inner_nodes = (m-1)*(n-1)\n",
    "    \n",
    "    first_gap = n-2\n",
    "    second_gap = (n-2) + (n-1)*(m-2)\n",
    "    \n",
    "    indices = np.zeros((4*num_inner_nodes, 2))\n",
    "    values = np.zeros((4*num_inner_nodes, ))\n",
    "    dense_size = np.array([num_inner_nodes, num_interactions]) \n",
    "    \n",
    "    current = 0\n",
    "    for i in range(num_inner_nodes):\n",
    "        indices[current][:] = np.array([i, i])\n",
    "        values[current] = 1.\n",
    "        current = current + 1\n",
    "        \n",
    "        indices[current][:] = np.array([i, i+1+first_gap])\n",
    "        values[current] = -1.\n",
    "        current = current + 1\n",
    "\n",
    "        indices[current][:] = np.array([i, i+1+first_gap+1+second_gap + int(i/(n-1))])\n",
    "        values[current] = 1.\n",
    "        current = current + 1\n",
    "\n",
    "        indices[current][:] = np.array([i, i+1+first_gap+1+second_gap+1 + int(i/(n-1))])\n",
    "        values[current] = -1.\n",
    "        current = current + 1\n",
    "\n",
    "    values = tf.convert_to_tensor(values, dtype='float32')\n",
    "    \n",
    "    PM = tf.sparse.SparseTensor(indices, values, dense_size)\n",
    "    \n",
    "    return PM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, nhu=2, npl=32):\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    model.add(tf.keras.layers.Input(input_shape))\n",
    "\n",
    "    for _ in range(nhu):\n",
    "        model.add(tf.keras.layers.Dense(npl, \n",
    "                                        activation='relu',\n",
    "                                        #kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.01)))\n",
    "                                        kernel_initializer='glorot_normal'))\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(1, activation='tanh'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshapes H into (Nx-2)*(Ny-2) matrix and add 0s for the boundary\n",
    "def reshape_H(Nx, Ny, A):\n",
    "    A = tf.reshape(A, (Nx-2, Ny-2))\n",
    "\n",
    "    #top and bottom in the xy plane\n",
    "    top_and_bottom = tf.zeros((Nx-2, 1), dtype='float32')\n",
    "\n",
    "    A = tf.concat([top_and_bottom, A, top_and_bottom], axis=1)\n",
    "\n",
    "    left_and_right = tf.zeros((1, Ny), dtype='float32')\n",
    "\n",
    "    A = tf.concat([left_and_right, A, left_and_right], axis=0)\n",
    "\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes the values of inner grid points and add boundary values\n",
    "def update_boundary(Nx, Ny, H):\n",
    "\n",
    "    #different values for different bounday points\n",
    "    x = np.linspace(xmin, xmax, Nx)\n",
    "    y = np.linspace(ymin, ymax, Ny)\n",
    "\n",
    "    T1 = np.sin(5*np.pi*y/2)\n",
    "    T2 = -x + 1\n",
    "    T3 = -4*(y - 1/2)**2 + 1\n",
    "    T4 = 0*x\n",
    "\n",
    "    #top and bottom in the xy plane\n",
    "    bottom = tf.convert_to_tensor(np.reshape(T4[1:-1], (Nx-2, 1)), dtype='float32')\n",
    "    top = tf.convert_to_tensor(np.reshape(T2[1:-1], (Nx-2, 1)), dtype='float32')\n",
    "    \n",
    "    output = tf.concat([bottom, H, top], axis=1)\n",
    "    \n",
    "    left = tf.convert_to_tensor(np.reshape(T1, (1, Ny)), dtype='float32')\n",
    "    right = tf.convert_to_tensor(np.reshape(T3, (1, Ny)), dtype='float32')\n",
    "    \n",
    "    output = tf.concat([left, output, right], axis=0)\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Energy Functional\n",
    "def compute_energy(phi):\n",
    "    #combination of forward and backward\n",
    "    grad_f = tf.math.square(phi[2:, 1:-1] - phi[1:-1, 1:-1])/(hx*hx) \\\n",
    "              + tf.math.square(phi[1:-1, 2:] - phi[1:-1, 1:-1])/(hy*hy)\n",
    "    \n",
    "    grad_b = tf.math.square(phi[1:-1, 1:-1] - phi[0:-2, 1:-1])/(hx*hx) \\\n",
    "              + tf.math.square(phi[1:-1, 1:-1] - phi[1:-1, 0:-2])/(hy*hy)\n",
    "            \n",
    "    grad2 = (grad_f + grad_b)/2.\n",
    "    \n",
    "    integral = 0.5*grad2\n",
    "\n",
    "    #riemann sum\n",
    "    energy = tf.reduce_sum(hx*hy*integral)\n",
    "    \n",
    "    return energy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Algorithm 2 from the paper\n",
    "#M is the number of message passing steps\n",
    "#fR and fO are the networks\n",
    "def march_forward(M, phi_in, fR, fO, PM):\n",
    "\n",
    "    Nx = phi_in.shape[0]\n",
    "    Ny = phi_in.shape[1]\n",
    "\n",
    "    H = tf.zeros(phi_in.shape, dtype='float32')\n",
    "    \n",
    "    for _ in range(M):\n",
    "        input_phi = organize_interaction(phi_in)\n",
    "        input_H = organize_interaction(H)\n",
    "        input1 = tf.concat([input_phi, input_H], axis=1)\n",
    "\n",
    "        \n",
    "        output1 = fR(input1)\n",
    "\n",
    "        c = tf.sparse.sparse_dense_matmul(PM, output1)\n",
    "        \n",
    "        input2 = tf.concat((tf.reshape(phi_in[1:-1, 1:-1], (-1, 1)), c), axis=1)\n",
    "        \n",
    "        output2 = fO(input2)\n",
    "        \n",
    "        H = reshape_H(Nx, Ny, output2)\n",
    "    \n",
    "    #adding the boundary values\n",
    "    phi_out = update_boundary(Nx, Ny, H[1:-1, 1:-1])\n",
    "    return phi_out, c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(M, phi_in, fR, fO, PM, initial=False, direct_training=False):\n",
    "    phi_out, _ = march_forward(M, phi_in, fR, fO, PM)\n",
    "    \n",
    "    energy_in = compute_energy(phi_in)\n",
    "    energy_out = compute_energy(phi_out)\n",
    "    \n",
    "    if initial:\n",
    "        loss = tf.reduce_mean(tf.math.square(phi_in - phi_out))\n",
    "    elif direct_training:\n",
    "        loss = energy_out\n",
    "    else:\n",
    "        loss = tf.math.reduce_mean(tf.math.square(phi_out - phi_in)) + omega*energy_out/energy_in\n",
    "    \n",
    "    return energy_in, energy_out, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_phi(Nx, Ny):\n",
    "\n",
    "    phi = np.zeros((Nx, Ny))\n",
    "\n",
    "    phi[1:-1, 1:-1] = np.random.uniform(0, 1, (Nx-2, Ny-2))\n",
    "\n",
    "    #different values for different bounday points\n",
    "    x = np.linspace(xmin, xmax, Nx)\n",
    "    y = np.linspace(ymin, ymax, Ny)\n",
    "\n",
    "    T1 = np.sin(5*np.pi*y/2)\n",
    "    T2 = -x + 1\n",
    "    T3 = -4*(y - 1/2)**2 + 1\n",
    "    T4 = 0*x\n",
    "\n",
    "    phi[:, 0]       = T4\n",
    "    phi[:, Ny-1]    = T3\n",
    "    phi[0, :]       = T1\n",
    "    phi[Nx-1, :]    = T2\n",
    "\n",
    "    return tf.convert_to_tensor(phi, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FONN():\n",
    "    def __init__(self, M):\n",
    "        self.M = M \n",
    "        self.PM = calc_PM(Nx, Ny)\n",
    "        \n",
    "        input_shape = (4, )\n",
    "\n",
    "        self.fR = create_model(input_shape=input_shape)\n",
    "        self.fO = create_model(input_shape=(2, ))\n",
    "        \n",
    "        self.phi_0 = initialize_phi(Nx, Ny)\n",
    "        self.phi   = initialize_phi(Nx, Ny)\n",
    "    \n",
    "    def initial_training(self, epochs):\n",
    "        optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "        for ep in range(epochs):\n",
    "            noise = tf.random.normal(self.phi_0.shape, mean=0.0, stddev=0.05, dtype='float32')\n",
    "            noisy_input = self.phi_0 + noise\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                _, _, loss = compute_loss(self.M, noisy_input, self.fR, self.fO, self.PM, initial=True)\n",
    "            grads = tape.gradient(loss, tape.watched_variables())\n",
    "            \n",
    "            optimizer.apply_gradients(zip(grads, tape.watched_variables()))\n",
    "            del tape\n",
    "\n",
    "            print(\"initial training step = \", ep, \" loss = \", loss.numpy())\n",
    "\n",
    "    #This function trains the networks using energy itself as the loss function\n",
    "    def direct_training(self, steps, save_plots=False, location=\"\"):\n",
    "        optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "        if save_plots:\n",
    "            name_Loss = location + \"/loss.dat\" \n",
    "            file_Loss = open(name_Loss, \"w\")\n",
    "        \n",
    "        for step in range(steps):\n",
    "            with tf.GradientTape() as tape:\n",
    "                energy_in, _, loss = compute_loss(self.M, self.phi, self.fR, self.fO, self.PM, direct_training=True)\n",
    "\n",
    "            grads = tape.gradient(loss, tape.watched_variables())\n",
    "\n",
    "            optimizer.apply_gradients(zip(grads, tape.watched_variables()))\n",
    "            del tape\n",
    "\n",
    "            print(\"step = \", step, \" loss = \", loss.numpy())\n",
    "\n",
    "            if save_plots:\n",
    "                result_E = str(loss.numpy()) + \"\\n\"\n",
    "                file_Loss.write(result_E)\n",
    "        \n",
    "        file_Loss.close()\n",
    "\n",
    "        output, _ = march_forward(self.M, self.phi, self.fR, self.fO, self.PM)\n",
    "        E_output = compute_energy(output)\n",
    "\n",
    "        if save_plots:\n",
    "            data_file_name = location + \"/initial.txt\"\n",
    "            np.savetxt(data_file_name, np.reshape(self.phi.numpy(), -1))\n",
    "\n",
    "            data_file_name = location + \"/final.txt\"\n",
    "            np.savetxt(data_file_name, np.reshape(output.numpy(), -1))\n",
    "            \n",
    "            fig_name = location + \"/initial.pdf\"\n",
    "            title_ = \"$E(\\phi)$ = \" + str(energy_in.numpy())\n",
    "            plt.figure()\n",
    "            plt.contourf(tf.transpose(self.phi))\n",
    "            plt.colorbar()\n",
    "            plt.title(title_)\n",
    "            plt.savefig(fig_name)\n",
    "            plt.close()\n",
    "\n",
    "            fig_name = location + \"/final.pdf\"\n",
    "            title_ = \"$E(\\phi)$ = \" + str(E_output.numpy())\n",
    "            plt.figure()\n",
    "            plt.contourf(tf.transpose(output))\n",
    "            plt.colorbar()\n",
    "            plt.title(title_)\n",
    "            plt.savefig(fig_name)\n",
    "            plt.close()\n",
    "\n",
    "    \n",
    "    #K -> fine tuning steps\n",
    "    def progressive_method(self, steps, K, save_plots=False, save_fRfO = False, location=\"\"):\n",
    "        optimizer = tf.keras.optimizers.Adam()\n",
    "        \n",
    "        #open the files\n",
    "        if save_plots:\n",
    "            name_E = location + \"/energy.txt\"\n",
    "            file_E = open(name_E, \"w\")\n",
    "\n",
    "            name_Loss = location + \"/loss.txt\"\n",
    "            file_Loss = open(name_Loss, \"w\")\n",
    "\n",
    "        for step in range(steps):\n",
    "            \n",
    "            for k in range(K):\n",
    "                with tf.GradientTape() as tape:\n",
    "                    energy_in, energy_out, loss = compute_loss(self.M, self.phi, self.fR, self.fO, self.PM)\n",
    "                \n",
    "                grads = tape.gradient(loss, tape.watched_variables())\n",
    "\n",
    "                optimizer.apply_gradients(zip(grads, tape.watched_variables()))\n",
    "                del tape\n",
    "\n",
    "                if save_plots:\n",
    "                    result_Loss = str(loss.numpy()) + \"\\n\"\n",
    "                    file_Loss.write(result_Loss)\n",
    "                        \n",
    "            self.phi, cij = march_forward(self.M, self.phi, self.fR, self.fO, self.PM)\n",
    "\n",
    "            print(\"step = \", step, \" energy_in = \", energy_in.numpy(), \" energy_out = \", energy_out.numpy())\n",
    "            \n",
    "\n",
    "            #To save plots and result\n",
    "            if save_plots:\n",
    "\n",
    "                result_E = str(energy_in.numpy()) + \"\\t\" + str(energy_out.numpy()) + \"\\n\"\n",
    "                file_E.write(result_E)\n",
    "                \n",
    "                #save figure\n",
    "                if step % 50 == 0:\n",
    "                    data_file_name = location + \"/step_\" + str(step) + \".txt\"\n",
    "                    np.savetxt(data_file_name, np.reshape(self.phi.numpy(), -1))\n",
    "                    fig_name = location + \"/\" + str(step) + \".pdf\"\n",
    "                    title_ = \"$E(u)$ = \" + str(compute_energy(self.phi).numpy())\n",
    "                    plt.figure()\n",
    "                    plt.contourf(tf.transpose(self.phi))\n",
    "                    plt.colorbar()\n",
    "                    plt.title(title_)\n",
    "                    plt.savefig(fig_name)\n",
    "                    plt.close()\n",
    "                        \n",
    "                    #plot cij\n",
    "                    data_file_name = location + \"/cij_\" + str(step) + \".txt\"\n",
    "                    np.savetxt(data_file_name, cij.numpy())\n",
    "                    fig_name = location + \"/cij_\" + str(step) + \".pdf\"\n",
    "                    plt.figure()\n",
    "                    plt.contourf(np.transpose(np.reshape(cij.numpy(), (Nx-2, Ny-2))))\n",
    "                    plt.colorbar()\n",
    "                    plt.savefig(fig_name)\n",
    "                    plt.close()\n",
    "\n",
    "                    if save_fRfO:\n",
    "                        self.fR.save(location + \"/model_fR_\" + str(step) + \".h5\")\n",
    "                        self.fO.save(location + \"/model_fO_\" + str(step) + \".h5\")\n",
    "                \n",
    "        if save_plots:\n",
    "            file_E.close()\n",
    "            file_Loss.close()\n",
    "            \n",
    "            E = np.genfromtxt(name_E, dtype='float32')\n",
    "            plt.figure()\n",
    "            plt.semilogy(E[:,0], linewidth=2)\n",
    "            plt.grid(visible=True, which='both')\n",
    "            plt.title(\"$E(\\phi)$\")\n",
    "            plt.savefig(location + \"/E.pdf\")\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#M = 1\n",
    "model = FONN(1)\n",
    "\n",
    "\n",
    "location = \"./Heat_Results\"\n",
    "if not os.path.exists(location):\n",
    "    os.makedirs(location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.initial_training(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.progressive_method(steps=501, K=10, save_plots=True, location=location)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "815a983d1180f45e43b0f9e623859d6adc5008144848ba59c486e4fdc876a1b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
