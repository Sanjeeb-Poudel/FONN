{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "\n",
    "#Number of divisions along x and y axis\n",
    "Nx = 101\n",
    "Ny = 101\n",
    "\n",
    "\n",
    "#grid size\n",
    "hx = 1/(Nx - 1)\n",
    "hy = 1/(Ny - 1)\n",
    "\n",
    "#domain size\n",
    "xmin = 0.0\n",
    "xmax = 1.0\n",
    "ymin = 0.0\n",
    "ymax = 1.0\n",
    "\n",
    "#weight \n",
    "omega = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function considers all the inner points and organize the nodal values for the calculation of e\n",
    "#input_shape = (Nx, Ny)\n",
    "#output_shape = (number of edges, 2)\n",
    "def organize_interaction(A):\n",
    "\n",
    "    first = tf.concat([tf.reshape(A[1:, 1:-1], (-1, 1)), tf.reshape(A[0:-1, 1:-1], (-1, 1))], axis=1)\n",
    "    \n",
    "    second = tf.concat([tf.reshape(A[1:-1, 1:], (-1, 1)), tf.reshape(A[1:-1, 0:-1], (-1, 1))], axis=1)\n",
    "\n",
    "    input = tf.concat([first, second], axis=0)\n",
    "    \n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multiplying the result of f(R) with PM matrix gives c_{i,j}\n",
    "def calc_PM(Nx, Ny):\n",
    "    #number of division along x and y axes\n",
    "    m = Nx - 1\n",
    "    n = Ny - 1\n",
    "\n",
    "    num_interactions = (n-1)*m + (m-1)*n\n",
    "    num_inner_nodes = (m-1)*(n-1)\n",
    "    \n",
    "    first_gap = n-2\n",
    "    second_gap = (n-2) + (n-1)*(m-2)\n",
    "    \n",
    "    indices = np.zeros((4*num_inner_nodes, 2))\n",
    "    values = np.zeros((4*num_inner_nodes, ))\n",
    "    dense_size = np.array([num_inner_nodes, num_interactions]) \n",
    "    \n",
    "    current = 0\n",
    "    for i in range(num_inner_nodes):\n",
    "        indices[current][:] = np.array([i, i])\n",
    "        values[current] = 1.\n",
    "        current = current + 1\n",
    "        \n",
    "        indices[current][:] = np.array([i, i+1+first_gap])\n",
    "        values[current] = -1.\n",
    "        current = current + 1\n",
    "\n",
    "        indices[current][:] = np.array([i, i+1+first_gap+1+second_gap + int(i/(n-1))])\n",
    "        values[current] = 1.\n",
    "        current = current + 1\n",
    "\n",
    "        indices[current][:] = np.array([i, i+1+first_gap+1+second_gap+1 + int(i/(n-1))])\n",
    "        values[current] = -1.\n",
    "        current = current + 1\n",
    "\n",
    "    values = tf.convert_to_tensor(values, dtype='float32')\n",
    "    \n",
    "    PM = tf.sparse.SparseTensor(indices, values, dense_size)\n",
    "    \n",
    "    return PM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, nhu=2, npl=32):\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    model.add(tf.keras.layers.Input(input_shape))\n",
    "\n",
    "    for _ in range(nhu):\n",
    "        model.add(tf.keras.layers.Dense(npl, \n",
    "                                        activation='relu',\n",
    "                                        #kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.01)))\n",
    "                                        kernel_initializer='glorot_normal'))\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(1, activation='tanh'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshapes H into (Nx-2)*(Ny-2) matrix and add 0s for the boundary\n",
    "def reshape_H(Nx, Ny, A):\n",
    "    A = tf.reshape(A, (Nx-2, Ny-2))\n",
    "\n",
    "    #top and bottom in the xy plane\n",
    "    top_and_bottom = tf.zeros((Nx-2, 1), dtype='float32')\n",
    "\n",
    "    A = tf.concat([top_and_bottom, A, top_and_bottom], axis=1)\n",
    "\n",
    "    left_and_right = tf.zeros((1, Ny), dtype='float32')\n",
    "\n",
    "    A = tf.concat([left_and_right, A, left_and_right], axis=0)\n",
    "\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes the values of inner grid points and add boundary values\n",
    "def update_boundary(Nx, Ny, H):\n",
    "\n",
    "    #different values for different bounday points\n",
    "    x = np.linspace(xmin, xmax, Nx)\n",
    "    y = np.linspace(ymin, ymax, Ny)\n",
    "\n",
    "    T1 = np.sin(5*np.pi*y/2)\n",
    "    T2 = -x + 1\n",
    "    T3 = -4*(y - 1/2)**2 + 1\n",
    "    T4 = 0*x\n",
    "\n",
    "    #top and bottom in the xy plane\n",
    "    bottom = tf.convert_to_tensor(np.reshape(T4[1:-1], (Nx-2, 1)), dtype='float32')\n",
    "    top = tf.convert_to_tensor(np.reshape(T2[1:-1], (Nx-2, 1)), dtype='float32')\n",
    "    \n",
    "    output = tf.concat([bottom, H, top], axis=1)\n",
    "    \n",
    "    left = tf.convert_to_tensor(np.reshape(T1, (1, Ny)), dtype='float32')\n",
    "    right = tf.convert_to_tensor(np.reshape(T3, (1, Ny)), dtype='float32')\n",
    "    \n",
    "    output = tf.concat([left, output, right], axis=0)\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Energy Functional\n",
    "def compute_energy(phi):\n",
    "    #combination of forward and backward\n",
    "    grad_f = tf.math.square(phi[2:, 1:-1] - phi[1:-1, 1:-1])/(hx*hx) \\\n",
    "              + tf.math.square(phi[1:-1, 2:] - phi[1:-1, 1:-1])/(hy*hy)\n",
    "    \n",
    "    grad_b = tf.math.square(phi[1:-1, 1:-1] - phi[0:-2, 1:-1])/(hx*hx) \\\n",
    "              + tf.math.square(phi[1:-1, 1:-1] - phi[1:-1, 0:-2])/(hy*hy)\n",
    "            \n",
    "    grad2 = (grad_f + grad_b)/2.\n",
    "    \n",
    "    integral = 0.5*grad2\n",
    "\n",
    "    #riemann sum\n",
    "    energy = tf.reduce_sum(hx*hy*integral)\n",
    "    \n",
    "    return energy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Algorithm 2 from the paper\n",
    "#M is the number of message passing steps\n",
    "#fR and fO are the networks\n",
    "def march_forward(M, phi_in, fR, fO, PM):\n",
    "\n",
    "    Nx = phi_in.shape[0]\n",
    "    Ny = phi_in.shape[1]\n",
    "\n",
    "    H = tf.zeros(phi_in.shape, dtype='float32')\n",
    "    \n",
    "    for _ in range(M):\n",
    "        input_phi = organize_interaction(phi_in)\n",
    "        input_H = organize_interaction(H)\n",
    "        input1 = tf.concat([input_phi, input_H], axis=1)\n",
    "\n",
    "        \n",
    "        output1 = fR(input1)\n",
    "\n",
    "        c = tf.sparse.sparse_dense_matmul(PM, output1)\n",
    "        \n",
    "        input2 = tf.concat((tf.reshape(phi_in[1:-1, 1:-1], (-1, 1)), c), axis=1)\n",
    "        \n",
    "        output2 = fO(input2)\n",
    "        \n",
    "        H = reshape_H(Nx, Ny, output2)\n",
    "    \n",
    "    #adding the boundary values\n",
    "    phi_out = update_boundary(Nx, Ny, H[1:-1, 1:-1])\n",
    "    return phi_out, c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(M, phi_in, fR, fO, PM, initial=False, direct_training=False):\n",
    "    phi_out, _ = march_forward(M, phi_in, fR, fO, PM)\n",
    "    \n",
    "    energy_in = compute_energy(phi_in)\n",
    "    energy_out = compute_energy(phi_out)\n",
    "    \n",
    "    if initial:\n",
    "        loss = tf.reduce_mean(tf.math.square(phi_in - phi_out))\n",
    "    elif direct_training:\n",
    "        loss = energy_out\n",
    "    else:\n",
    "        loss = tf.math.reduce_mean(tf.math.square(phi_out - phi_in)) + omega*energy_out/energy_in\n",
    "    \n",
    "    return energy_in, energy_out, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_phi(Nx, Ny):\n",
    "\n",
    "    phi = np.zeros((Nx, Ny))\n",
    "\n",
    "    phi[1:-1, 1:-1] = np.random.uniform(0, 1, (Nx-2, Ny-2))\n",
    "\n",
    "    #different values for different bounday points\n",
    "    x = np.linspace(xmin, xmax, Nx)\n",
    "    y = np.linspace(ymin, ymax, Ny)\n",
    "\n",
    "    T1 = np.sin(5*np.pi*y/2)\n",
    "    T2 = -x + 1\n",
    "    T3 = -4*(y - 1/2)**2 + 1\n",
    "    T4 = 0*x\n",
    "\n",
    "    phi[:, 0]       = T4\n",
    "    phi[:, Ny-1]    = T3\n",
    "    phi[0, :]       = T1\n",
    "    phi[Nx-1, :]    = T2\n",
    "\n",
    "    return tf.convert_to_tensor(phi, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FONN():\n",
    "    def __init__(self, M):\n",
    "        self.M = M \n",
    "        self.PM = calc_PM(Nx, Ny)\n",
    "        \n",
    "        input_shape = (4, )\n",
    "\n",
    "        self.fR = create_model(input_shape=input_shape)\n",
    "        self.fO = create_model(input_shape=(2, ))\n",
    "        \n",
    "        self.phi_0 = initialize_phi(Nx, Ny)\n",
    "        self.phi   = initialize_phi(Nx, Ny)\n",
    "    \n",
    "    def initial_training(self, epochs):\n",
    "        optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "        for ep in range(epochs):\n",
    "            if ep != epochs-1:\n",
    "                noise = tf.random.normal(self.phi_0.shape, mean=0.0, stddev=0.05, dtype='float32')\n",
    "                noisy_input = self.phi_0 + noise\n",
    "            elif ep == epochs - 1:\n",
    "                noisy_input = self.phi_0\n",
    "            with tf.GradientTape() as tape:\n",
    "                _, _, loss = compute_loss(self.M, noisy_input, self.fR, self.fO, self.PM, initial=True)\n",
    "            grads = tape.gradient(loss, tape.watched_variables())\n",
    "            \n",
    "            optimizer.apply_gradients(zip(grads, tape.watched_variables()))\n",
    "            print(\"initial training step = \", ep, \" loss = \", loss.numpy())\n",
    "\n",
    "    #This function trains the networks using energy itself as the loss function\n",
    "    def direct_training(self, steps, save_plots=False, location=\"\"):\n",
    "        optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "        if save_plots:\n",
    "            name_Loss = location + \"/loss.dat\" \n",
    "            file_Loss = open(name_Loss, \"w\")\n",
    "        \n",
    "        for step in range(steps):\n",
    "            with tf.GradientTape() as tape:\n",
    "                energy_in, _, loss = compute_loss(self.M, self.phi, self.fR, self.fO, self.PM, direct_training=True)\n",
    "\n",
    "            grads = tape.gradient(loss, tape.watched_variables())\n",
    "\n",
    "            optimizer.apply_gradients(zip(grads, tape.watched_variables()))\n",
    "            del tape\n",
    "\n",
    "            print(\"step = \", step, \" loss = \", loss.numpy())\n",
    "\n",
    "            if save_plots:\n",
    "                result_E = str(loss.numpy()) + \"\\n\"\n",
    "                file_Loss.write(result_E)\n",
    "        \n",
    "        file_Loss.close()\n",
    "\n",
    "        output, _ = march_forward(self.M, self.phi, self.fR, self.fO, self.PM)\n",
    "        E_output = compute_energy(output)\n",
    "\n",
    "        if save_plots:\n",
    "            data_file_name = location + \"/initial.txt\"\n",
    "            np.savetxt(data_file_name, np.reshape(self.phi.numpy(), -1))\n",
    "\n",
    "            data_file_name = location + \"/final.txt\"\n",
    "            np.savetxt(data_file_name, np.reshape(output.numpy(), -1))\n",
    "            \n",
    "            fig_name = location + \"/initial.pdf\"\n",
    "            title_ = \"$E(\\phi)$ = \" + str(energy_in.numpy())\n",
    "            plt.figure()\n",
    "            plt.contourf(tf.transpose(self.phi))\n",
    "            plt.colorbar()\n",
    "            plt.title(title_)\n",
    "            plt.savefig(fig_name)\n",
    "            plt.close()\n",
    "\n",
    "            fig_name = location + \"/final.pdf\"\n",
    "            title_ = \"$E(\\phi)$ = \" + str(E_output.numpy())\n",
    "            plt.figure()\n",
    "            plt.contourf(tf.transpose(output))\n",
    "            plt.colorbar()\n",
    "            plt.title(title_)\n",
    "            plt.savefig(fig_name)\n",
    "            plt.close()\n",
    "\n",
    "    \n",
    "    #K -> fine tuning steps\n",
    "    def progressive_method(self, steps, K, save_plots=False, save_fRfO = False, location=\"\"):\n",
    "        optimizer = tf.keras.optimizers.Adam()\n",
    "        \n",
    "        #open the files\n",
    "        if save_plots:\n",
    "            name_E = location + \"/energy.txt\"\n",
    "            file_E = open(name_E, \"w\")\n",
    "\n",
    "            name_Loss = location + \"/loss.txt\"\n",
    "            file_Loss = open(name_Loss, \"w\")\n",
    "\n",
    "        for step in range(steps):\n",
    "            \n",
    "            for k in range(K):\n",
    "                with tf.GradientTape() as tape:\n",
    "                    energy_in, energy_out, loss = compute_loss(self.M, self.phi, self.fR, self.fO, self.PM)\n",
    "                \n",
    "                grads = tape.gradient(loss, tape.watched_variables())\n",
    "\n",
    "                optimizer.apply_gradients(zip(grads, tape.watched_variables()))\n",
    "                del tape\n",
    "\n",
    "                if save_plots:\n",
    "                    result_Loss = str(loss.numpy()) + \"\\n\"\n",
    "                    file_Loss.write(result_Loss)\n",
    "                        \n",
    "            self.phi, cij = march_forward(self.M, self.phi, self.fR, self.fO, self.PM)\n",
    "\n",
    "            print(\"step = \", step, \" energy_in = \", energy_in.numpy(), \" energy_out = \", energy_out.numpy())\n",
    "            \n",
    "\n",
    "            #To save plots and result\n",
    "            if save_plots:\n",
    "\n",
    "                result_E = str(energy_in.numpy()) + \"\\t\" + str(energy_out.numpy()) + \"\\n\"\n",
    "                file_E.write(result_E)\n",
    "                \n",
    "                #save figure\n",
    "                if step % 50 == 0:\n",
    "                    data_file_name = location + \"/step_\" + str(step) + \".txt\"\n",
    "                    np.savetxt(data_file_name, np.reshape(self.phi.numpy(), -1))\n",
    "                    fig_name = location + \"/\" + str(step) + \".pdf\"\n",
    "                    title_ = \"$E(u)$ = \" + str(energy_in.numpy())\n",
    "                    plt.figure()\n",
    "                    plt.contourf(tf.transpose(self.phi))\n",
    "                    plt.colorbar()\n",
    "                    plt.title(title_)\n",
    "                    plt.savefig(fig_name)\n",
    "                    plt.close()\n",
    "                        \n",
    "                    #plot cij\n",
    "                    data_file_name = location + \"/cij_\" + str(step) + \".txt\"\n",
    "                    np.savetxt(data_file_name, cij.numpy())\n",
    "                    fig_name = location + \"/cij_\" + str(step) + \".pdf\"\n",
    "                    plt.figure()\n",
    "                    plt.contourf(np.transpose(np.reshape(cij.numpy(), (Nx-2, Ny-2))))\n",
    "                    plt.colorbar()\n",
    "                    plt.savefig(fig_name)\n",
    "                    plt.close()\n",
    "\n",
    "                    if save_fRfO:\n",
    "                        self.fR.save(location + \"/model_fR_\" + str(step) + \".h5\")\n",
    "                        self.fO.save(location + \"/model_fO_\" + str(step) + \".h5\")\n",
    "                \n",
    "        if save_plots:\n",
    "            file_E.close()\n",
    "            file_Loss.close()\n",
    "            \n",
    "            E = np.genfromtxt(name_E, dtype='float32')\n",
    "            plt.figure()\n",
    "            plt.semilogy(E[:,0], linewidth=2)\n",
    "            plt.grid(visible=True, which='both')\n",
    "            plt.title(\"$E(\\phi)$\")\n",
    "            plt.savefig(location + \"/E.pdf\")\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial training step =  0  loss =  0.31709984\n",
      "initial training step =  1  loss =  0.29409084\n",
      "initial training step =  2  loss =  0.27249733\n",
      "initial training step =  3  loss =  0.2527558\n",
      "initial training step =  4  loss =  0.23522127\n",
      "initial training step =  5  loss =  0.21725519\n",
      "initial training step =  6  loss =  0.20203848\n",
      "initial training step =  7  loss =  0.18780065\n",
      "initial training step =  8  loss =  0.1733513\n",
      "initial training step =  9  loss =  0.1598999\n",
      "initial training step =  10  loss =  0.14736131\n",
      "initial training step =  11  loss =  0.13762653\n",
      "initial training step =  12  loss =  0.12746467\n",
      "initial training step =  13  loss =  0.1176569\n",
      "initial training step =  14  loss =  0.10940377\n",
      "initial training step =  15  loss =  0.1013\n",
      "initial training step =  16  loss =  0.09361408\n",
      "initial training step =  17  loss =  0.08633626\n",
      "initial training step =  18  loss =  0.07967948\n",
      "initial training step =  19  loss =  0.07376034\n",
      "initial training step =  20  loss =  0.06795018\n",
      "initial training step =  21  loss =  0.06224654\n",
      "initial training step =  22  loss =  0.05774308\n",
      "initial training step =  23  loss =  0.052566424\n",
      "initial training step =  24  loss =  0.048585724\n",
      "initial training step =  25  loss =  0.044770434\n",
      "initial training step =  26  loss =  0.04091497\n",
      "initial training step =  27  loss =  0.0379667\n",
      "initial training step =  28  loss =  0.035056595\n",
      "initial training step =  29  loss =  0.032010633\n",
      "initial training step =  30  loss =  0.0299918\n",
      "initial training step =  31  loss =  0.027559841\n",
      "initial training step =  32  loss =  0.025657807\n",
      "initial training step =  33  loss =  0.02401458\n",
      "initial training step =  34  loss =  0.022201838\n",
      "initial training step =  35  loss =  0.020454831\n",
      "initial training step =  36  loss =  0.019343834\n",
      "initial training step =  37  loss =  0.018364098\n",
      "initial training step =  38  loss =  0.017136209\n",
      "initial training step =  39  loss =  0.016210975\n",
      "initial training step =  40  loss =  0.01517446\n",
      "initial training step =  41  loss =  0.014598349\n",
      "initial training step =  42  loss =  0.013821573\n",
      "initial training step =  43  loss =  0.013099308\n",
      "initial training step =  44  loss =  0.012681787\n",
      "initial training step =  45  loss =  0.012241411\n",
      "initial training step =  46  loss =  0.011695856\n",
      "initial training step =  47  loss =  0.011314079\n",
      "initial training step =  48  loss =  0.010738612\n",
      "initial training step =  49  loss =  0.010578934\n",
      "initial training step =  50  loss =  0.010203372\n",
      "initial training step =  51  loss =  0.009941633\n",
      "initial training step =  52  loss =  0.009736835\n",
      "initial training step =  53  loss =  0.00957233\n",
      "initial training step =  54  loss =  0.009232503\n",
      "initial training step =  55  loss =  0.00915923\n",
      "initial training step =  56  loss =  0.00901841\n",
      "initial training step =  57  loss =  0.008780928\n",
      "initial training step =  58  loss =  0.008671691\n",
      "initial training step =  59  loss =  0.008625653\n",
      "initial training step =  60  loss =  0.00833515\n",
      "initial training step =  61  loss =  0.0084011685\n",
      "initial training step =  62  loss =  0.008153445\n",
      "initial training step =  63  loss =  0.008079305\n",
      "initial training step =  64  loss =  0.007989519\n",
      "initial training step =  65  loss =  0.007984445\n",
      "initial training step =  66  loss =  0.0078109317\n",
      "initial training step =  67  loss =  0.00783612\n",
      "initial training step =  68  loss =  0.007769494\n",
      "initial training step =  69  loss =  0.007666348\n",
      "initial training step =  70  loss =  0.0077151824\n",
      "initial training step =  71  loss =  0.0076086833\n",
      "initial training step =  72  loss =  0.0074838693\n",
      "initial training step =  73  loss =  0.0073290854\n",
      "initial training step =  74  loss =  0.0071927905\n",
      "initial training step =  75  loss =  0.007265624\n",
      "initial training step =  76  loss =  0.0071811997\n",
      "initial training step =  77  loss =  0.007134666\n",
      "initial training step =  78  loss =  0.007199429\n",
      "initial training step =  79  loss =  0.0070512425\n",
      "initial training step =  80  loss =  0.007058537\n",
      "initial training step =  81  loss =  0.006989158\n",
      "initial training step =  82  loss =  0.0068972562\n",
      "initial training step =  83  loss =  0.0067279893\n",
      "initial training step =  84  loss =  0.0068582604\n",
      "initial training step =  85  loss =  0.0067286277\n",
      "initial training step =  86  loss =  0.006835317\n",
      "initial training step =  87  loss =  0.006665313\n",
      "initial training step =  88  loss =  0.006644457\n",
      "initial training step =  89  loss =  0.0066628074\n",
      "initial training step =  90  loss =  0.006686968\n",
      "initial training step =  91  loss =  0.006589435\n",
      "initial training step =  92  loss =  0.0066589573\n",
      "initial training step =  93  loss =  0.006444494\n",
      "initial training step =  94  loss =  0.0064337207\n",
      "initial training step =  95  loss =  0.0065509956\n",
      "initial training step =  96  loss =  0.006382912\n",
      "initial training step =  97  loss =  0.0064263586\n",
      "initial training step =  98  loss =  0.006377848\n",
      "initial training step =  99  loss =  0.0061477977\n",
      "initial training step =  100  loss =  0.006203998\n",
      "initial training step =  101  loss =  0.0061911843\n",
      "initial training step =  102  loss =  0.0061353445\n",
      "initial training step =  103  loss =  0.006067771\n",
      "initial training step =  104  loss =  0.00612643\n",
      "initial training step =  105  loss =  0.006143432\n",
      "initial training step =  106  loss =  0.0060642418\n",
      "initial training step =  107  loss =  0.0058624404\n",
      "initial training step =  108  loss =  0.0059496327\n",
      "initial training step =  109  loss =  0.0058449623\n",
      "initial training step =  110  loss =  0.0059028175\n",
      "initial training step =  111  loss =  0.005811874\n",
      "initial training step =  112  loss =  0.0058358824\n",
      "initial training step =  113  loss =  0.0059282817\n",
      "initial training step =  114  loss =  0.005766301\n",
      "initial training step =  115  loss =  0.0056992616\n",
      "initial training step =  116  loss =  0.0056783617\n",
      "initial training step =  117  loss =  0.0057045734\n",
      "initial training step =  118  loss =  0.0057169055\n",
      "initial training step =  119  loss =  0.005572573\n",
      "initial training step =  120  loss =  0.0057213213\n",
      "initial training step =  121  loss =  0.0056386893\n",
      "initial training step =  122  loss =  0.0055628116\n",
      "initial training step =  123  loss =  0.0054433723\n",
      "initial training step =  124  loss =  0.0054753562\n",
      "initial training step =  125  loss =  0.005437241\n",
      "initial training step =  126  loss =  0.0056310305\n",
      "initial training step =  127  loss =  0.0055495384\n",
      "initial training step =  128  loss =  0.0053998656\n",
      "initial training step =  129  loss =  0.0053381305\n",
      "initial training step =  130  loss =  0.0053860447\n",
      "initial training step =  131  loss =  0.005355792\n",
      "initial training step =  132  loss =  0.00549928\n",
      "initial training step =  133  loss =  0.0053561083\n",
      "initial training step =  134  loss =  0.0054036705\n",
      "initial training step =  135  loss =  0.005320501\n",
      "initial training step =  136  loss =  0.0053188577\n",
      "initial training step =  137  loss =  0.0052668448\n",
      "initial training step =  138  loss =  0.0052981908\n",
      "initial training step =  139  loss =  0.0053384406\n",
      "initial training step =  140  loss =  0.005312108\n",
      "initial training step =  141  loss =  0.0052074385\n",
      "initial training step =  142  loss =  0.0051973797\n",
      "initial training step =  143  loss =  0.005266663\n",
      "initial training step =  144  loss =  0.0052775275\n",
      "initial training step =  145  loss =  0.0051638796\n",
      "initial training step =  146  loss =  0.0051910616\n",
      "initial training step =  147  loss =  0.005101813\n",
      "initial training step =  148  loss =  0.0051695486\n",
      "initial training step =  149  loss =  0.0051331474\n",
      "initial training step =  150  loss =  0.0052290563\n",
      "initial training step =  151  loss =  0.005111384\n",
      "initial training step =  152  loss =  0.005235576\n",
      "initial training step =  153  loss =  0.0050109765\n",
      "initial training step =  154  loss =  0.005133378\n",
      "initial training step =  155  loss =  0.004987666\n",
      "initial training step =  156  loss =  0.0051238965\n",
      "initial training step =  157  loss =  0.005155321\n",
      "initial training step =  158  loss =  0.005036879\n",
      "initial training step =  159  loss =  0.0050070817\n",
      "initial training step =  160  loss =  0.005096759\n",
      "initial training step =  161  loss =  0.004976633\n",
      "initial training step =  162  loss =  0.0050537144\n",
      "initial training step =  163  loss =  0.00502304\n",
      "initial training step =  164  loss =  0.0049226996\n",
      "initial training step =  165  loss =  0.0049571157\n",
      "initial training step =  166  loss =  0.0049215034\n",
      "initial training step =  167  loss =  0.004944906\n",
      "initial training step =  168  loss =  0.004953831\n",
      "initial training step =  169  loss =  0.004980676\n",
      "initial training step =  170  loss =  0.0049953684\n",
      "initial training step =  171  loss =  0.0049365293\n",
      "initial training step =  172  loss =  0.0048142923\n",
      "initial training step =  173  loss =  0.004921967\n",
      "initial training step =  174  loss =  0.0048794146\n",
      "initial training step =  175  loss =  0.0048882896\n",
      "initial training step =  176  loss =  0.0049152034\n",
      "initial training step =  177  loss =  0.0048734494\n",
      "initial training step =  178  loss =  0.0050364095\n",
      "initial training step =  179  loss =  0.00489006\n",
      "initial training step =  180  loss =  0.0049792244\n",
      "initial training step =  181  loss =  0.0049579437\n",
      "initial training step =  182  loss =  0.004802177\n",
      "initial training step =  183  loss =  0.004808173\n",
      "initial training step =  184  loss =  0.004932033\n",
      "initial training step =  185  loss =  0.0048625674\n",
      "initial training step =  186  loss =  0.004856894\n",
      "initial training step =  187  loss =  0.0048091593\n",
      "initial training step =  188  loss =  0.0049099466\n",
      "initial training step =  189  loss =  0.0048371907\n",
      "initial training step =  190  loss =  0.004912778\n",
      "initial training step =  191  loss =  0.004872247\n",
      "initial training step =  192  loss =  0.0046795267\n",
      "initial training step =  193  loss =  0.0047710566\n",
      "initial training step =  194  loss =  0.0048422273\n",
      "initial training step =  195  loss =  0.0048198528\n",
      "initial training step =  196  loss =  0.004718259\n",
      "initial training step =  197  loss =  0.00469318\n",
      "initial training step =  198  loss =  0.00471204\n",
      "initial training step =  199  loss =  0.004802075\n",
      "initial training step =  200  loss =  0.0047075287\n",
      "initial training step =  201  loss =  0.004827013\n",
      "initial training step =  202  loss =  0.004858824\n",
      "initial training step =  203  loss =  0.004738246\n",
      "initial training step =  204  loss =  0.004739062\n",
      "initial training step =  205  loss =  0.004797502\n",
      "initial training step =  206  loss =  0.0047481474\n",
      "initial training step =  207  loss =  0.004724628\n",
      "initial training step =  208  loss =  0.0046844655\n",
      "initial training step =  209  loss =  0.004827263\n",
      "initial training step =  210  loss =  0.004699654\n",
      "initial training step =  211  loss =  0.0048199687\n",
      "initial training step =  212  loss =  0.0047361017\n",
      "initial training step =  213  loss =  0.004758348\n",
      "initial training step =  214  loss =  0.004782815\n",
      "initial training step =  215  loss =  0.0047589056\n",
      "initial training step =  216  loss =  0.004837989\n",
      "initial training step =  217  loss =  0.0047524604\n",
      "initial training step =  218  loss =  0.0047919247\n",
      "initial training step =  219  loss =  0.0046281153\n",
      "initial training step =  220  loss =  0.0046444205\n",
      "initial training step =  221  loss =  0.0047428235\n",
      "initial training step =  222  loss =  0.004622283\n",
      "initial training step =  223  loss =  0.0045755105\n",
      "initial training step =  224  loss =  0.004720383\n",
      "initial training step =  225  loss =  0.0047140685\n",
      "initial training step =  226  loss =  0.0045971684\n",
      "initial training step =  227  loss =  0.004573622\n",
      "initial training step =  228  loss =  0.00451509\n",
      "initial training step =  229  loss =  0.004618788\n",
      "initial training step =  230  loss =  0.004636347\n",
      "initial training step =  231  loss =  0.004772087\n",
      "initial training step =  232  loss =  0.004680382\n",
      "initial training step =  233  loss =  0.0046223234\n",
      "initial training step =  234  loss =  0.0045540477\n",
      "initial training step =  235  loss =  0.004675322\n",
      "initial training step =  236  loss =  0.0045552547\n",
      "initial training step =  237  loss =  0.004592993\n",
      "initial training step =  238  loss =  0.0045767445\n",
      "initial training step =  239  loss =  0.0046376646\n",
      "initial training step =  240  loss =  0.004581799\n",
      "initial training step =  241  loss =  0.004663971\n",
      "initial training step =  242  loss =  0.004681426\n",
      "initial training step =  243  loss =  0.004601029\n",
      "initial training step =  244  loss =  0.0046159024\n",
      "initial training step =  245  loss =  0.0046199337\n",
      "initial training step =  246  loss =  0.0046154847\n",
      "initial training step =  247  loss =  0.0045360085\n",
      "initial training step =  248  loss =  0.0045327046\n",
      "initial training step =  249  loss =  0.0046331226\n",
      "initial training step =  250  loss =  0.0046240054\n",
      "initial training step =  251  loss =  0.0045482763\n",
      "initial training step =  252  loss =  0.0045960452\n",
      "initial training step =  253  loss =  0.004548996\n",
      "initial training step =  254  loss =  0.004565208\n",
      "initial training step =  255  loss =  0.0046489364\n",
      "initial training step =  256  loss =  0.0044042487\n",
      "initial training step =  257  loss =  0.004490102\n",
      "initial training step =  258  loss =  0.0045400304\n",
      "initial training step =  259  loss =  0.004516551\n",
      "initial training step =  260  loss =  0.0045140106\n",
      "initial training step =  261  loss =  0.0045035775\n",
      "initial training step =  262  loss =  0.004625006\n",
      "initial training step =  263  loss =  0.004515376\n",
      "initial training step =  264  loss =  0.004671866\n",
      "initial training step =  265  loss =  0.004523506\n",
      "initial training step =  266  loss =  0.004573447\n",
      "initial training step =  267  loss =  0.004511222\n",
      "initial training step =  268  loss =  0.004522259\n",
      "initial training step =  269  loss =  0.0045896093\n",
      "initial training step =  270  loss =  0.0045944294\n",
      "initial training step =  271  loss =  0.0045253867\n",
      "initial training step =  272  loss =  0.0044750716\n",
      "initial training step =  273  loss =  0.004464916\n",
      "initial training step =  274  loss =  0.004494677\n",
      "initial training step =  275  loss =  0.004502312\n",
      "initial training step =  276  loss =  0.004523957\n",
      "initial training step =  277  loss =  0.0044930894\n",
      "initial training step =  278  loss =  0.004501641\n",
      "initial training step =  279  loss =  0.00458612\n",
      "initial training step =  280  loss =  0.004572022\n",
      "initial training step =  281  loss =  0.0046350774\n",
      "initial training step =  282  loss =  0.0045986716\n",
      "initial training step =  283  loss =  0.00448357\n",
      "initial training step =  284  loss =  0.0044999793\n",
      "initial training step =  285  loss =  0.004510287\n",
      "initial training step =  286  loss =  0.004487752\n",
      "initial training step =  287  loss =  0.0044701393\n",
      "initial training step =  288  loss =  0.0045146905\n",
      "initial training step =  289  loss =  0.004418098\n",
      "initial training step =  290  loss =  0.0044778017\n",
      "initial training step =  291  loss =  0.0044900384\n",
      "initial training step =  292  loss =  0.0046660975\n",
      "initial training step =  293  loss =  0.004451593\n",
      "initial training step =  294  loss =  0.0044543785\n",
      "initial training step =  295  loss =  0.004366504\n",
      "initial training step =  296  loss =  0.0044426164\n",
      "initial training step =  297  loss =  0.004400859\n",
      "initial training step =  298  loss =  0.004468456\n",
      "initial training step =  299  loss =  0.0045602964\n",
      "initial training step =  300  loss =  0.004463018\n",
      "initial training step =  301  loss =  0.0044637606\n",
      "initial training step =  302  loss =  0.004506769\n",
      "initial training step =  303  loss =  0.0044844286\n",
      "initial training step =  304  loss =  0.0043520704\n",
      "initial training step =  305  loss =  0.004514481\n",
      "initial training step =  306  loss =  0.0044967146\n",
      "initial training step =  307  loss =  0.0044957595\n",
      "initial training step =  308  loss =  0.0044900095\n",
      "initial training step =  309  loss =  0.0043499065\n",
      "initial training step =  310  loss =  0.004493852\n",
      "initial training step =  311  loss =  0.0044131074\n",
      "initial training step =  312  loss =  0.0043750675\n",
      "initial training step =  313  loss =  0.004501065\n",
      "initial training step =  314  loss =  0.004446024\n",
      "initial training step =  315  loss =  0.004370074\n",
      "initial training step =  316  loss =  0.004382484\n",
      "initial training step =  317  loss =  0.004519431\n",
      "initial training step =  318  loss =  0.0044577913\n",
      "initial training step =  319  loss =  0.004619749\n",
      "initial training step =  320  loss =  0.004403707\n",
      "initial training step =  321  loss =  0.00428552\n",
      "initial training step =  322  loss =  0.004428124\n",
      "initial training step =  323  loss =  0.004517526\n",
      "initial training step =  324  loss =  0.004447036\n",
      "initial training step =  325  loss =  0.0043235538\n",
      "initial training step =  326  loss =  0.004455745\n",
      "initial training step =  327  loss =  0.004368879\n",
      "initial training step =  328  loss =  0.0043427036\n",
      "initial training step =  329  loss =  0.00441904\n",
      "initial training step =  330  loss =  0.0042573195\n",
      "initial training step =  331  loss =  0.0044349846\n",
      "initial training step =  332  loss =  0.004373564\n",
      "initial training step =  333  loss =  0.0043540928\n",
      "initial training step =  334  loss =  0.004364801\n",
      "initial training step =  335  loss =  0.0043569026\n",
      "initial training step =  336  loss =  0.0044067446\n",
      "initial training step =  337  loss =  0.0043816566\n",
      "initial training step =  338  loss =  0.0044185803\n",
      "initial training step =  339  loss =  0.004384165\n",
      "initial training step =  340  loss =  0.0042455774\n",
      "initial training step =  341  loss =  0.004342578\n",
      "initial training step =  342  loss =  0.0043912884\n",
      "initial training step =  343  loss =  0.004389394\n",
      "initial training step =  344  loss =  0.0043731895\n",
      "initial training step =  345  loss =  0.004274592\n",
      "initial training step =  346  loss =  0.0044241156\n",
      "initial training step =  347  loss =  0.004478644\n",
      "initial training step =  348  loss =  0.0044104615\n",
      "initial training step =  349  loss =  0.004304295\n",
      "initial training step =  350  loss =  0.004359247\n",
      "initial training step =  351  loss =  0.0043646106\n",
      "initial training step =  352  loss =  0.004451519\n",
      "initial training step =  353  loss =  0.004359994\n",
      "initial training step =  354  loss =  0.004276629\n",
      "initial training step =  355  loss =  0.004384841\n",
      "initial training step =  356  loss =  0.004395699\n",
      "initial training step =  357  loss =  0.0043472187\n",
      "initial training step =  358  loss =  0.004249453\n",
      "initial training step =  359  loss =  0.0041572857\n",
      "initial training step =  360  loss =  0.0044039893\n",
      "initial training step =  361  loss =  0.004229883\n",
      "initial training step =  362  loss =  0.004317562\n",
      "initial training step =  363  loss =  0.004259623\n",
      "initial training step =  364  loss =  0.0042228065\n",
      "initial training step =  365  loss =  0.004265558\n",
      "initial training step =  366  loss =  0.004363115\n",
      "initial training step =  367  loss =  0.004397018\n",
      "initial training step =  368  loss =  0.0043590013\n",
      "initial training step =  369  loss =  0.004359687\n",
      "initial training step =  370  loss =  0.004236115\n",
      "initial training step =  371  loss =  0.004309907\n",
      "initial training step =  372  loss =  0.004202334\n",
      "initial training step =  373  loss =  0.0045278687\n",
      "initial training step =  374  loss =  0.004326639\n",
      "initial training step =  375  loss =  0.0042867684\n",
      "initial training step =  376  loss =  0.004292053\n",
      "initial training step =  377  loss =  0.004327152\n",
      "initial training step =  378  loss =  0.0042464216\n",
      "initial training step =  379  loss =  0.0044408045\n",
      "initial training step =  380  loss =  0.0043505137\n",
      "initial training step =  381  loss =  0.004325175\n",
      "initial training step =  382  loss =  0.004372449\n",
      "initial training step =  383  loss =  0.004341261\n",
      "initial training step =  384  loss =  0.004364539\n",
      "initial training step =  385  loss =  0.004331344\n",
      "initial training step =  386  loss =  0.00425054\n",
      "initial training step =  387  loss =  0.0043214527\n",
      "initial training step =  388  loss =  0.0043214853\n",
      "initial training step =  389  loss =  0.0042587854\n",
      "initial training step =  390  loss =  0.004331736\n",
      "initial training step =  391  loss =  0.004290797\n",
      "initial training step =  392  loss =  0.004313794\n",
      "initial training step =  393  loss =  0.0042236173\n",
      "initial training step =  394  loss =  0.0043128743\n",
      "initial training step =  395  loss =  0.004357747\n",
      "initial training step =  396  loss =  0.004285924\n",
      "initial training step =  397  loss =  0.0043161213\n",
      "initial training step =  398  loss =  0.0043299636\n",
      "initial training step =  399  loss =  0.004278012\n",
      "initial training step =  400  loss =  0.004182277\n",
      "initial training step =  401  loss =  0.004298022\n",
      "initial training step =  402  loss =  0.0042989356\n",
      "initial training step =  403  loss =  0.004262603\n",
      "initial training step =  404  loss =  0.004210635\n",
      "initial training step =  405  loss =  0.004183096\n",
      "initial training step =  406  loss =  0.004351972\n",
      "initial training step =  407  loss =  0.0042221365\n",
      "initial training step =  408  loss =  0.004213717\n",
      "initial training step =  409  loss =  0.004329137\n",
      "initial training step =  410  loss =  0.0042420244\n",
      "initial training step =  411  loss =  0.0042999345\n",
      "initial training step =  412  loss =  0.0042904206\n",
      "initial training step =  413  loss =  0.004292294\n",
      "initial training step =  414  loss =  0.0041217217\n",
      "initial training step =  415  loss =  0.0042437403\n",
      "initial training step =  416  loss =  0.0042356174\n",
      "initial training step =  417  loss =  0.0042651384\n",
      "initial training step =  418  loss =  0.0042628534\n",
      "initial training step =  419  loss =  0.0043948893\n",
      "initial training step =  420  loss =  0.0043578586\n",
      "initial training step =  421  loss =  0.0042399494\n",
      "initial training step =  422  loss =  0.0043072538\n",
      "initial training step =  423  loss =  0.0042970506\n",
      "initial training step =  424  loss =  0.0042418754\n",
      "initial training step =  425  loss =  0.0043123174\n",
      "initial training step =  426  loss =  0.004241318\n",
      "initial training step =  427  loss =  0.004354922\n",
      "initial training step =  428  loss =  0.004257191\n",
      "initial training step =  429  loss =  0.0042479597\n",
      "initial training step =  430  loss =  0.004226672\n",
      "initial training step =  431  loss =  0.00428533\n",
      "initial training step =  432  loss =  0.0042725275\n",
      "initial training step =  433  loss =  0.004184694\n",
      "initial training step =  434  loss =  0.0042249863\n",
      "initial training step =  435  loss =  0.004252904\n",
      "initial training step =  436  loss =  0.004188518\n",
      "initial training step =  437  loss =  0.0041542933\n",
      "initial training step =  438  loss =  0.0043132138\n",
      "initial training step =  439  loss =  0.004131867\n",
      "initial training step =  440  loss =  0.004261316\n",
      "initial training step =  441  loss =  0.0042526443\n",
      "initial training step =  442  loss =  0.0043259454\n",
      "initial training step =  443  loss =  0.0042275833\n",
      "initial training step =  444  loss =  0.0041637802\n",
      "initial training step =  445  loss =  0.004291523\n",
      "initial training step =  446  loss =  0.004185399\n",
      "initial training step =  447  loss =  0.0041931905\n",
      "initial training step =  448  loss =  0.0042665703\n",
      "initial training step =  449  loss =  0.004149043\n",
      "initial training step =  450  loss =  0.004328712\n",
      "initial training step =  451  loss =  0.0042161527\n",
      "initial training step =  452  loss =  0.004216632\n",
      "initial training step =  453  loss =  0.0042632967\n",
      "initial training step =  454  loss =  0.0041965563\n",
      "initial training step =  455  loss =  0.0041764854\n",
      "initial training step =  456  loss =  0.004255166\n",
      "initial training step =  457  loss =  0.0042394777\n",
      "initial training step =  458  loss =  0.0041989074\n",
      "initial training step =  459  loss =  0.0042825807\n",
      "initial training step =  460  loss =  0.004141334\n",
      "initial training step =  461  loss =  0.0043408284\n",
      "initial training step =  462  loss =  0.0041599656\n",
      "initial training step =  463  loss =  0.0040658326\n",
      "initial training step =  464  loss =  0.004106623\n",
      "initial training step =  465  loss =  0.004232029\n",
      "initial training step =  466  loss =  0.004099393\n",
      "initial training step =  467  loss =  0.0041516894\n",
      "initial training step =  468  loss =  0.004242236\n",
      "initial training step =  469  loss =  0.0042482237\n",
      "initial training step =  470  loss =  0.004271442\n",
      "initial training step =  471  loss =  0.004231233\n",
      "initial training step =  472  loss =  0.0041590356\n",
      "initial training step =  473  loss =  0.0041602724\n",
      "initial training step =  474  loss =  0.0041491427\n",
      "initial training step =  475  loss =  0.0042112092\n",
      "initial training step =  476  loss =  0.0040366715\n",
      "initial training step =  477  loss =  0.0042002536\n",
      "initial training step =  478  loss =  0.0039692977\n",
      "initial training step =  479  loss =  0.0042399904\n",
      "initial training step =  480  loss =  0.004208401\n",
      "initial training step =  481  loss =  0.0042055957\n",
      "initial training step =  482  loss =  0.0042222277\n",
      "initial training step =  483  loss =  0.004213399\n",
      "initial training step =  484  loss =  0.004190575\n",
      "initial training step =  485  loss =  0.0041092103\n",
      "initial training step =  486  loss =  0.0042758817\n",
      "initial training step =  487  loss =  0.0041729542\n",
      "initial training step =  488  loss =  0.0042297593\n",
      "initial training step =  489  loss =  0.004235274\n",
      "initial training step =  490  loss =  0.004092708\n",
      "initial training step =  491  loss =  0.004204002\n",
      "initial training step =  492  loss =  0.004038411\n",
      "initial training step =  493  loss =  0.004080971\n",
      "initial training step =  494  loss =  0.0043132454\n",
      "initial training step =  495  loss =  0.004260638\n",
      "initial training step =  496  loss =  0.0041891346\n",
      "initial training step =  497  loss =  0.004124057\n",
      "initial training step =  498  loss =  0.004108944\n",
      "initial training step =  499  loss =  0.004094504\n",
      "initial training step =  500  loss =  0.004114613\n",
      "initial training step =  501  loss =  0.004123198\n",
      "initial training step =  502  loss =  0.0041550454\n",
      "initial training step =  503  loss =  0.004253108\n",
      "initial training step =  504  loss =  0.004009415\n",
      "initial training step =  505  loss =  0.004150104\n",
      "initial training step =  506  loss =  0.004175466\n",
      "initial training step =  507  loss =  0.004095631\n",
      "initial training step =  508  loss =  0.0041389884\n",
      "initial training step =  509  loss =  0.0041740825\n",
      "initial training step =  510  loss =  0.0041272645\n",
      "initial training step =  511  loss =  0.004142785\n",
      "initial training step =  512  loss =  0.004102961\n",
      "initial training step =  513  loss =  0.004095507\n",
      "initial training step =  514  loss =  0.004299681\n",
      "initial training step =  515  loss =  0.004104921\n",
      "initial training step =  516  loss =  0.0042871684\n",
      "initial training step =  517  loss =  0.0041536223\n",
      "initial training step =  518  loss =  0.0040295054\n",
      "initial training step =  519  loss =  0.0041217315\n",
      "initial training step =  520  loss =  0.0041887285\n",
      "initial training step =  521  loss =  0.0040553934\n",
      "initial training step =  522  loss =  0.0040368824\n",
      "initial training step =  523  loss =  0.004202635\n",
      "initial training step =  524  loss =  0.0041302503\n",
      "initial training step =  525  loss =  0.0042182016\n",
      "initial training step =  526  loss =  0.004212565\n",
      "initial training step =  527  loss =  0.004249938\n",
      "initial training step =  528  loss =  0.0041969125\n",
      "initial training step =  529  loss =  0.00417813\n",
      "initial training step =  530  loss =  0.0041396767\n",
      "initial training step =  531  loss =  0.0041646445\n",
      "initial training step =  532  loss =  0.004126578\n",
      "initial training step =  533  loss =  0.0040370803\n",
      "initial training step =  534  loss =  0.004075418\n",
      "initial training step =  535  loss =  0.004020507\n",
      "initial training step =  536  loss =  0.004134652\n",
      "initial training step =  537  loss =  0.0040835496\n",
      "initial training step =  538  loss =  0.0041218186\n",
      "initial training step =  539  loss =  0.00418469\n",
      "initial training step =  540  loss =  0.0040933136\n",
      "initial training step =  541  loss =  0.0041465983\n",
      "initial training step =  542  loss =  0.0041300356\n",
      "initial training step =  543  loss =  0.004146834\n",
      "initial training step =  544  loss =  0.0041350205\n",
      "initial training step =  545  loss =  0.004236614\n",
      "initial training step =  546  loss =  0.0040673655\n",
      "initial training step =  547  loss =  0.0041435906\n",
      "initial training step =  548  loss =  0.0040753544\n",
      "initial training step =  549  loss =  0.00419219\n",
      "initial training step =  550  loss =  0.0040883212\n",
      "initial training step =  551  loss =  0.0040575867\n",
      "initial training step =  552  loss =  0.0039386423\n",
      "initial training step =  553  loss =  0.004051477\n",
      "initial training step =  554  loss =  0.004175827\n",
      "initial training step =  555  loss =  0.004183327\n",
      "initial training step =  556  loss =  0.004012456\n",
      "initial training step =  557  loss =  0.0040745423\n",
      "initial training step =  558  loss =  0.004059525\n",
      "initial training step =  559  loss =  0.00418012\n",
      "initial training step =  560  loss =  0.0040934375\n",
      "initial training step =  561  loss =  0.0040727886\n",
      "initial training step =  562  loss =  0.0041512405\n",
      "initial training step =  563  loss =  0.0041584084\n",
      "initial training step =  564  loss =  0.0040955055\n",
      "initial training step =  565  loss =  0.0040597254\n",
      "initial training step =  566  loss =  0.0040320517\n",
      "initial training step =  567  loss =  0.004077935\n",
      "initial training step =  568  loss =  0.0041331314\n",
      "initial training step =  569  loss =  0.004098106\n",
      "initial training step =  570  loss =  0.0042138044\n",
      "initial training step =  571  loss =  0.0041661183\n",
      "initial training step =  572  loss =  0.0040174993\n",
      "initial training step =  573  loss =  0.0040965728\n",
      "initial training step =  574  loss =  0.004090218\n",
      "initial training step =  575  loss =  0.0042894376\n",
      "initial training step =  576  loss =  0.004069545\n",
      "initial training step =  577  loss =  0.004090803\n",
      "initial training step =  578  loss =  0.004038434\n",
      "initial training step =  579  loss =  0.004195169\n",
      "initial training step =  580  loss =  0.004126964\n",
      "initial training step =  581  loss =  0.0041088443\n",
      "initial training step =  582  loss =  0.0040780604\n",
      "initial training step =  583  loss =  0.0041516325\n",
      "initial training step =  584  loss =  0.0041115023\n",
      "initial training step =  585  loss =  0.00414531\n",
      "initial training step =  586  loss =  0.004116984\n",
      "initial training step =  587  loss =  0.0041045747\n",
      "initial training step =  588  loss =  0.0039577032\n",
      "initial training step =  589  loss =  0.00412689\n",
      "initial training step =  590  loss =  0.0040616225\n",
      "initial training step =  591  loss =  0.0041123773\n",
      "initial training step =  592  loss =  0.0041993456\n",
      "initial training step =  593  loss =  0.0040519442\n",
      "initial training step =  594  loss =  0.0041890494\n",
      "initial training step =  595  loss =  0.004126781\n",
      "initial training step =  596  loss =  0.0041055055\n",
      "initial training step =  597  loss =  0.0041203704\n",
      "initial training step =  598  loss =  0.0041005686\n",
      "initial training step =  599  loss =  0.004064199\n",
      "initial training step =  600  loss =  0.0040622456\n",
      "initial training step =  601  loss =  0.004049825\n",
      "initial training step =  602  loss =  0.0040380675\n",
      "initial training step =  603  loss =  0.0040599387\n",
      "initial training step =  604  loss =  0.0040697395\n",
      "initial training step =  605  loss =  0.003971169\n",
      "initial training step =  606  loss =  0.0041133575\n",
      "initial training step =  607  loss =  0.004050526\n",
      "initial training step =  608  loss =  0.0041137105\n",
      "initial training step =  609  loss =  0.004098936\n",
      "initial training step =  610  loss =  0.004051305\n",
      "initial training step =  611  loss =  0.0041246894\n",
      "initial training step =  612  loss =  0.004098482\n",
      "initial training step =  613  loss =  0.004076184\n",
      "initial training step =  614  loss =  0.0041866465\n",
      "initial training step =  615  loss =  0.004164259\n",
      "initial training step =  616  loss =  0.0040726294\n",
      "initial training step =  617  loss =  0.004091677\n",
      "initial training step =  618  loss =  0.004164263\n",
      "initial training step =  619  loss =  0.004084501\n",
      "initial training step =  620  loss =  0.004098683\n",
      "initial training step =  621  loss =  0.004077591\n",
      "initial training step =  622  loss =  0.003992787\n",
      "initial training step =  623  loss =  0.003993478\n",
      "initial training step =  624  loss =  0.0041315816\n",
      "initial training step =  625  loss =  0.0040756613\n",
      "initial training step =  626  loss =  0.004212174\n",
      "initial training step =  627  loss =  0.0041318987\n",
      "initial training step =  628  loss =  0.0041607725\n",
      "initial training step =  629  loss =  0.004051059\n",
      "initial training step =  630  loss =  0.0040712454\n",
      "initial training step =  631  loss =  0.004329195\n",
      "initial training step =  632  loss =  0.0039778263\n",
      "initial training step =  633  loss =  0.004093808\n",
      "initial training step =  634  loss =  0.0041795415\n",
      "initial training step =  635  loss =  0.004197991\n",
      "initial training step =  636  loss =  0.004088689\n",
      "initial training step =  637  loss =  0.0040628132\n",
      "initial training step =  638  loss =  0.0041245464\n",
      "initial training step =  639  loss =  0.004226952\n",
      "initial training step =  640  loss =  0.004110546\n",
      "initial training step =  641  loss =  0.00410248\n",
      "initial training step =  642  loss =  0.0041217823\n",
      "initial training step =  643  loss =  0.004045244\n",
      "initial training step =  644  loss =  0.0040626866\n",
      "initial training step =  645  loss =  0.004191315\n",
      "initial training step =  646  loss =  0.004195354\n",
      "initial training step =  647  loss =  0.0040377923\n",
      "initial training step =  648  loss =  0.0041338904\n",
      "initial training step =  649  loss =  0.0041156905\n",
      "initial training step =  650  loss =  0.004073961\n",
      "initial training step =  651  loss =  0.0039772843\n",
      "initial training step =  652  loss =  0.0039471956\n",
      "initial training step =  653  loss =  0.004077918\n",
      "initial training step =  654  loss =  0.0040463656\n",
      "initial training step =  655  loss =  0.0040883673\n",
      "initial training step =  656  loss =  0.0040514204\n",
      "initial training step =  657  loss =  0.0040426045\n",
      "initial training step =  658  loss =  0.0039968463\n",
      "initial training step =  659  loss =  0.004009886\n",
      "initial training step =  660  loss =  0.003996628\n",
      "initial training step =  661  loss =  0.004030858\n",
      "initial training step =  662  loss =  0.0041713845\n",
      "initial training step =  663  loss =  0.0041041835\n",
      "initial training step =  664  loss =  0.00401667\n",
      "initial training step =  665  loss =  0.004188398\n",
      "initial training step =  666  loss =  0.0039910893\n",
      "initial training step =  667  loss =  0.004121941\n",
      "initial training step =  668  loss =  0.004102138\n",
      "initial training step =  669  loss =  0.0041002943\n",
      "initial training step =  670  loss =  0.0040159808\n",
      "initial training step =  671  loss =  0.00406838\n",
      "initial training step =  672  loss =  0.004087939\n",
      "initial training step =  673  loss =  0.004171978\n",
      "initial training step =  674  loss =  0.0042382204\n",
      "initial training step =  675  loss =  0.0040845233\n",
      "initial training step =  676  loss =  0.004166858\n",
      "initial training step =  677  loss =  0.004073057\n",
      "initial training step =  678  loss =  0.004235227\n",
      "initial training step =  679  loss =  0.0039825323\n",
      "initial training step =  680  loss =  0.004079266\n",
      "initial training step =  681  loss =  0.0041959262\n",
      "initial training step =  682  loss =  0.0040614936\n",
      "initial training step =  683  loss =  0.0041596526\n",
      "initial training step =  684  loss =  0.004136777\n",
      "initial training step =  685  loss =  0.003969105\n",
      "initial training step =  686  loss =  0.003991062\n",
      "initial training step =  687  loss =  0.0039654267\n",
      "initial training step =  688  loss =  0.004054207\n",
      "initial training step =  689  loss =  0.0039972435\n",
      "initial training step =  690  loss =  0.0041275634\n",
      "initial training step =  691  loss =  0.004144281\n",
      "initial training step =  692  loss =  0.004080651\n",
      "initial training step =  693  loss =  0.004028783\n",
      "initial training step =  694  loss =  0.004161936\n",
      "initial training step =  695  loss =  0.0040831915\n",
      "initial training step =  696  loss =  0.0040345504\n",
      "initial training step =  697  loss =  0.0041295104\n",
      "initial training step =  698  loss =  0.0039494657\n",
      "initial training step =  699  loss =  0.0040809414\n",
      "initial training step =  700  loss =  0.0040728566\n",
      "initial training step =  701  loss =  0.004174218\n",
      "initial training step =  702  loss =  0.004083633\n",
      "initial training step =  703  loss =  0.004037649\n",
      "initial training step =  704  loss =  0.0041408916\n",
      "initial training step =  705  loss =  0.0040626223\n",
      "initial training step =  706  loss =  0.004098862\n",
      "initial training step =  707  loss =  0.0040731467\n",
      "initial training step =  708  loss =  0.0040778797\n",
      "initial training step =  709  loss =  0.004008798\n",
      "initial training step =  710  loss =  0.00409578\n",
      "initial training step =  711  loss =  0.0040351762\n",
      "initial training step =  712  loss =  0.004114967\n",
      "initial training step =  713  loss =  0.0041156923\n",
      "initial training step =  714  loss =  0.004193581\n",
      "initial training step =  715  loss =  0.004011312\n",
      "initial training step =  716  loss =  0.0041199564\n",
      "initial training step =  717  loss =  0.00408281\n",
      "initial training step =  718  loss =  0.0041408367\n",
      "initial training step =  719  loss =  0.003954959\n",
      "initial training step =  720  loss =  0.0040580984\n",
      "initial training step =  721  loss =  0.00412945\n",
      "initial training step =  722  loss =  0.0040399553\n",
      "initial training step =  723  loss =  0.0042223195\n",
      "initial training step =  724  loss =  0.004114445\n",
      "initial training step =  725  loss =  0.0040897303\n",
      "initial training step =  726  loss =  0.0041293236\n",
      "initial training step =  727  loss =  0.0041249325\n",
      "initial training step =  728  loss =  0.0040913243\n",
      "initial training step =  729  loss =  0.0041575558\n",
      "initial training step =  730  loss =  0.004235042\n",
      "initial training step =  731  loss =  0.0041169547\n",
      "initial training step =  732  loss =  0.003987837\n",
      "initial training step =  733  loss =  0.004039233\n",
      "initial training step =  734  loss =  0.0040729176\n",
      "initial training step =  735  loss =  0.0040170997\n",
      "initial training step =  736  loss =  0.0040273964\n",
      "initial training step =  737  loss =  0.0041714977\n",
      "initial training step =  738  loss =  0.004071439\n",
      "initial training step =  739  loss =  0.004015217\n",
      "initial training step =  740  loss =  0.0041530957\n",
      "initial training step =  741  loss =  0.004161247\n",
      "initial training step =  742  loss =  0.004069915\n",
      "initial training step =  743  loss =  0.0039329967\n",
      "initial training step =  744  loss =  0.0042153723\n",
      "initial training step =  745  loss =  0.0039702407\n",
      "initial training step =  746  loss =  0.0040864353\n",
      "initial training step =  747  loss =  0.0040320596\n",
      "initial training step =  748  loss =  0.004017783\n",
      "initial training step =  749  loss =  0.004028699\n",
      "initial training step =  750  loss =  0.0040750643\n",
      "initial training step =  751  loss =  0.004046889\n",
      "initial training step =  752  loss =  0.004035004\n",
      "initial training step =  753  loss =  0.00410751\n",
      "initial training step =  754  loss =  0.0041809683\n",
      "initial training step =  755  loss =  0.0040252726\n",
      "initial training step =  756  loss =  0.0041398983\n",
      "initial training step =  757  loss =  0.004114832\n",
      "initial training step =  758  loss =  0.004028061\n",
      "initial training step =  759  loss =  0.0040299636\n",
      "initial training step =  760  loss =  0.004106965\n",
      "initial training step =  761  loss =  0.004070993\n",
      "initial training step =  762  loss =  0.0041266563\n",
      "initial training step =  763  loss =  0.0041331383\n",
      "initial training step =  764  loss =  0.004216104\n",
      "initial training step =  765  loss =  0.0040979655\n",
      "initial training step =  766  loss =  0.004099185\n",
      "initial training step =  767  loss =  0.0040059453\n",
      "initial training step =  768  loss =  0.004094208\n",
      "initial training step =  769  loss =  0.0040434795\n",
      "initial training step =  770  loss =  0.00408053\n",
      "initial training step =  771  loss =  0.0040913904\n",
      "initial training step =  772  loss =  0.004023269\n",
      "initial training step =  773  loss =  0.0040599965\n",
      "initial training step =  774  loss =  0.0039946954\n",
      "initial training step =  775  loss =  0.0041308403\n",
      "initial training step =  776  loss =  0.004136891\n",
      "initial training step =  777  loss =  0.004054595\n",
      "initial training step =  778  loss =  0.0040966794\n",
      "initial training step =  779  loss =  0.0041486756\n",
      "initial training step =  780  loss =  0.0041247387\n",
      "initial training step =  781  loss =  0.0040411972\n",
      "initial training step =  782  loss =  0.0041675717\n",
      "initial training step =  783  loss =  0.0041116583\n",
      "initial training step =  784  loss =  0.0039841738\n",
      "initial training step =  785  loss =  0.004158883\n",
      "initial training step =  786  loss =  0.004117312\n",
      "initial training step =  787  loss =  0.003976863\n",
      "initial training step =  788  loss =  0.0041940077\n",
      "initial training step =  789  loss =  0.0040939376\n",
      "initial training step =  790  loss =  0.004098109\n",
      "initial training step =  791  loss =  0.004103885\n",
      "initial training step =  792  loss =  0.0041336557\n",
      "initial training step =  793  loss =  0.004203894\n",
      "initial training step =  794  loss =  0.004104424\n",
      "initial training step =  795  loss =  0.004024066\n",
      "initial training step =  796  loss =  0.0040274817\n",
      "initial training step =  797  loss =  0.00414776\n",
      "initial training step =  798  loss =  0.0040787333\n",
      "initial training step =  799  loss =  0.0041156546\n",
      "initial training step =  800  loss =  0.004016383\n",
      "initial training step =  801  loss =  0.004042378\n",
      "initial training step =  802  loss =  0.004154741\n",
      "initial training step =  803  loss =  0.0040686387\n",
      "initial training step =  804  loss =  0.0039404375\n",
      "initial training step =  805  loss =  0.0039896253\n",
      "initial training step =  806  loss =  0.0040138904\n",
      "initial training step =  807  loss =  0.0041317632\n",
      "initial training step =  808  loss =  0.004223722\n",
      "initial training step =  809  loss =  0.0041523306\n",
      "initial training step =  810  loss =  0.0041375095\n",
      "initial training step =  811  loss =  0.004018638\n",
      "initial training step =  812  loss =  0.0041848733\n",
      "initial training step =  813  loss =  0.004156186\n",
      "initial training step =  814  loss =  0.0041451897\n",
      "initial training step =  815  loss =  0.004111296\n",
      "initial training step =  816  loss =  0.004015225\n",
      "initial training step =  817  loss =  0.004149497\n",
      "initial training step =  818  loss =  0.0041148905\n",
      "initial training step =  819  loss =  0.004128825\n",
      "initial training step =  820  loss =  0.0040060617\n",
      "initial training step =  821  loss =  0.0041250405\n",
      "initial training step =  822  loss =  0.0041159187\n",
      "initial training step =  823  loss =  0.0040772087\n",
      "initial training step =  824  loss =  0.004078317\n",
      "initial training step =  825  loss =  0.0040976205\n",
      "initial training step =  826  loss =  0.004081593\n",
      "initial training step =  827  loss =  0.0040919194\n",
      "initial training step =  828  loss =  0.0041090413\n",
      "initial training step =  829  loss =  0.004141427\n",
      "initial training step =  830  loss =  0.0041698096\n",
      "initial training step =  831  loss =  0.00408731\n",
      "initial training step =  832  loss =  0.0040957727\n",
      "initial training step =  833  loss =  0.004163288\n",
      "initial training step =  834  loss =  0.0040470795\n",
      "initial training step =  835  loss =  0.0041072476\n",
      "initial training step =  836  loss =  0.004101788\n",
      "initial training step =  837  loss =  0.004081088\n",
      "initial training step =  838  loss =  0.003995385\n",
      "initial training step =  839  loss =  0.0040902835\n",
      "initial training step =  840  loss =  0.004145218\n",
      "initial training step =  841  loss =  0.004156582\n",
      "initial training step =  842  loss =  0.004105534\n",
      "initial training step =  843  loss =  0.0040664007\n",
      "initial training step =  844  loss =  0.0040867236\n",
      "initial training step =  845  loss =  0.0040765065\n",
      "initial training step =  846  loss =  0.0040821135\n",
      "initial training step =  847  loss =  0.0039677466\n",
      "initial training step =  848  loss =  0.0041464567\n",
      "initial training step =  849  loss =  0.0040607704\n",
      "initial training step =  850  loss =  0.004108521\n",
      "initial training step =  851  loss =  0.0040567163\n",
      "initial training step =  852  loss =  0.00398389\n",
      "initial training step =  853  loss =  0.0040788585\n",
      "initial training step =  854  loss =  0.004126603\n",
      "initial training step =  855  loss =  0.003980024\n",
      "initial training step =  856  loss =  0.004137594\n",
      "initial training step =  857  loss =  0.0039467867\n",
      "initial training step =  858  loss =  0.004095293\n",
      "initial training step =  859  loss =  0.0039317785\n",
      "initial training step =  860  loss =  0.0041103936\n",
      "initial training step =  861  loss =  0.0040569194\n",
      "initial training step =  862  loss =  0.004045377\n",
      "initial training step =  863  loss =  0.0040430236\n",
      "initial training step =  864  loss =  0.0040731\n",
      "initial training step =  865  loss =  0.004037023\n",
      "initial training step =  866  loss =  0.0039758533\n",
      "initial training step =  867  loss =  0.0040602456\n",
      "initial training step =  868  loss =  0.004011365\n",
      "initial training step =  869  loss =  0.0040643336\n",
      "initial training step =  870  loss =  0.0039717294\n",
      "initial training step =  871  loss =  0.004153498\n",
      "initial training step =  872  loss =  0.00397735\n",
      "initial training step =  873  loss =  0.0041726124\n",
      "initial training step =  874  loss =  0.0041158604\n",
      "initial training step =  875  loss =  0.0040234565\n",
      "initial training step =  876  loss =  0.003991296\n",
      "initial training step =  877  loss =  0.0040363306\n",
      "initial training step =  878  loss =  0.0039923806\n",
      "initial training step =  879  loss =  0.003957735\n",
      "initial training step =  880  loss =  0.0039475076\n",
      "initial training step =  881  loss =  0.004084911\n",
      "initial training step =  882  loss =  0.0040416983\n",
      "initial training step =  883  loss =  0.0041112686\n",
      "initial training step =  884  loss =  0.0041750986\n",
      "initial training step =  885  loss =  0.004148536\n",
      "initial training step =  886  loss =  0.004025179\n",
      "initial training step =  887  loss =  0.0040463856\n",
      "initial training step =  888  loss =  0.0040031965\n",
      "initial training step =  889  loss =  0.0040910803\n",
      "initial training step =  890  loss =  0.0041311923\n",
      "initial training step =  891  loss =  0.0040444992\n",
      "initial training step =  892  loss =  0.0041637085\n",
      "initial training step =  893  loss =  0.0039390945\n",
      "initial training step =  894  loss =  0.0040996443\n",
      "initial training step =  895  loss =  0.004058118\n",
      "initial training step =  896  loss =  0.0040717674\n",
      "initial training step =  897  loss =  0.0041175215\n",
      "initial training step =  898  loss =  0.00415008\n",
      "initial training step =  899  loss =  0.0041514505\n",
      "initial training step =  900  loss =  0.0041758614\n",
      "initial training step =  901  loss =  0.0041216128\n",
      "initial training step =  902  loss =  0.0040717265\n",
      "initial training step =  903  loss =  0.0040360866\n",
      "initial training step =  904  loss =  0.0040936274\n",
      "initial training step =  905  loss =  0.0041648843\n",
      "initial training step =  906  loss =  0.004061084\n",
      "initial training step =  907  loss =  0.004017545\n",
      "initial training step =  908  loss =  0.0041186092\n",
      "initial training step =  909  loss =  0.0040885275\n",
      "initial training step =  910  loss =  0.0040398217\n",
      "initial training step =  911  loss =  0.004080687\n",
      "initial training step =  912  loss =  0.0040704766\n",
      "initial training step =  913  loss =  0.0042016627\n",
      "initial training step =  914  loss =  0.00411848\n",
      "initial training step =  915  loss =  0.0040254095\n",
      "initial training step =  916  loss =  0.0040523508\n",
      "initial training step =  917  loss =  0.0040612184\n",
      "initial training step =  918  loss =  0.0041274936\n",
      "initial training step =  919  loss =  0.0040672333\n",
      "initial training step =  920  loss =  0.0040046396\n",
      "initial training step =  921  loss =  0.0040262\n",
      "initial training step =  922  loss =  0.0040644025\n",
      "initial training step =  923  loss =  0.0040758224\n",
      "initial training step =  924  loss =  0.0041602463\n",
      "initial training step =  925  loss =  0.004247519\n",
      "initial training step =  926  loss =  0.0041958974\n",
      "initial training step =  927  loss =  0.004092185\n",
      "initial training step =  928  loss =  0.004074525\n",
      "initial training step =  929  loss =  0.0040124627\n",
      "initial training step =  930  loss =  0.004150261\n",
      "initial training step =  931  loss =  0.0041204677\n",
      "initial training step =  932  loss =  0.0040311697\n",
      "initial training step =  933  loss =  0.0040056496\n",
      "initial training step =  934  loss =  0.004022477\n",
      "initial training step =  935  loss =  0.003981159\n",
      "initial training step =  936  loss =  0.004080991\n",
      "initial training step =  937  loss =  0.004078244\n",
      "initial training step =  938  loss =  0.0041075847\n",
      "initial training step =  939  loss =  0.0040265312\n",
      "initial training step =  940  loss =  0.0040294523\n",
      "initial training step =  941  loss =  0.0041666995\n",
      "initial training step =  942  loss =  0.0039896173\n",
      "initial training step =  943  loss =  0.0040625376\n",
      "initial training step =  944  loss =  0.0041733123\n",
      "initial training step =  945  loss =  0.0041792076\n",
      "initial training step =  946  loss =  0.004058513\n",
      "initial training step =  947  loss =  0.0040979804\n",
      "initial training step =  948  loss =  0.0041546277\n",
      "initial training step =  949  loss =  0.0041118707\n",
      "initial training step =  950  loss =  0.004033587\n",
      "initial training step =  951  loss =  0.004102185\n",
      "initial training step =  952  loss =  0.004053589\n",
      "initial training step =  953  loss =  0.004070144\n",
      "initial training step =  954  loss =  0.004073597\n",
      "initial training step =  955  loss =  0.004031354\n",
      "initial training step =  956  loss =  0.0040612277\n",
      "initial training step =  957  loss =  0.0041212887\n",
      "initial training step =  958  loss =  0.0041336683\n",
      "initial training step =  959  loss =  0.004213996\n",
      "initial training step =  960  loss =  0.0040958053\n",
      "initial training step =  961  loss =  0.0041236067\n",
      "initial training step =  962  loss =  0.004090356\n",
      "initial training step =  963  loss =  0.004104004\n",
      "initial training step =  964  loss =  0.004000681\n",
      "initial training step =  965  loss =  0.0040678508\n",
      "initial training step =  966  loss =  0.0041368403\n",
      "initial training step =  967  loss =  0.003978763\n",
      "initial training step =  968  loss =  0.0040294467\n",
      "initial training step =  969  loss =  0.0040234043\n",
      "initial training step =  970  loss =  0.0039925193\n",
      "initial training step =  971  loss =  0.0040536234\n",
      "initial training step =  972  loss =  0.004245273\n",
      "initial training step =  973  loss =  0.0040349923\n",
      "initial training step =  974  loss =  0.0039735693\n",
      "initial training step =  975  loss =  0.00425711\n",
      "initial training step =  976  loss =  0.003960522\n",
      "initial training step =  977  loss =  0.004048839\n",
      "initial training step =  978  loss =  0.0040948913\n",
      "initial training step =  979  loss =  0.0041250233\n",
      "initial training step =  980  loss =  0.0041072755\n",
      "initial training step =  981  loss =  0.004133627\n",
      "initial training step =  982  loss =  0.003915977\n",
      "initial training step =  983  loss =  0.0039849007\n",
      "initial training step =  984  loss =  0.004056494\n",
      "initial training step =  985  loss =  0.004032363\n",
      "initial training step =  986  loss =  0.004106856\n",
      "initial training step =  987  loss =  0.0041123014\n",
      "initial training step =  988  loss =  0.004089575\n",
      "initial training step =  989  loss =  0.0040925327\n",
      "initial training step =  990  loss =  0.004176721\n",
      "initial training step =  991  loss =  0.0040290467\n",
      "initial training step =  992  loss =  0.004139004\n",
      "initial training step =  993  loss =  0.00404617\n",
      "initial training step =  994  loss =  0.0040796404\n",
      "initial training step =  995  loss =  0.004224227\n",
      "initial training step =  996  loss =  0.0039492548\n",
      "initial training step =  997  loss =  0.004084955\n",
      "initial training step =  998  loss =  0.0040947744\n",
      "initial training step =  999  loss =  0.0039390945\n"
     ]
    }
   ],
   "source": [
    "#M = 1\n",
    "model = FONN(1)\n",
    "\n",
    "\n",
    "location = \"./Heat_Results\"\n",
    "if not os.path.exists(location):\n",
    "    os.makedirs(location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.initial_training(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step =  0  energy_in =  1649.696  energy_out =  644.8546\n",
      "step =  1  energy_in =  571.5658  energy_out =  64.866745\n",
      "step =  2  energy_in =  62.173584  energy_out =  22.303146\n",
      "step =  3  energy_in =  21.968395  energy_out =  16.420223\n",
      "step =  4  energy_in =  16.552595  energy_out =  14.577039\n",
      "step =  5  energy_in =  14.33518  energy_out =  13.044805\n",
      "step =  6  energy_in =  12.977532  energy_out =  11.089842\n",
      "step =  7  energy_in =  11.054459  energy_out =  10.045473\n",
      "step =  8  energy_in =  10.012866  energy_out =  9.037835\n",
      "step =  9  energy_in =  9.017457  energy_out =  8.357686\n",
      "step =  10  energy_in =  8.346363  energy_out =  7.778018\n",
      "step =  11  energy_in =  7.767302  energy_out =  7.304523\n",
      "step =  12  energy_in =  7.294103  energy_out =  6.8453846\n",
      "step =  13  energy_in =  6.8339357  energy_out =  6.4332156\n",
      "step =  14  energy_in =  6.4296103  energy_out =  6.1164603\n",
      "step =  15  energy_in =  6.1121426  energy_out =  5.846147\n",
      "step =  16  energy_in =  5.843101  energy_out =  5.6179204\n",
      "step =  17  energy_in =  5.615187  energy_out =  5.4083467\n",
      "step =  18  energy_in =  5.4057145  energy_out =  5.2493973\n",
      "step =  19  energy_in =  5.2466154  energy_out =  5.117512\n",
      "step =  20  energy_in =  5.11305  energy_out =  5.014517\n",
      "step =  21  energy_in =  5.0120864  energy_out =  4.905336\n",
      "step =  22  energy_in =  4.900226  energy_out =  4.7968946\n",
      "step =  23  energy_in =  4.7952557  energy_out =  4.7036815\n",
      "step =  24  energy_in =  4.7007465  energy_out =  4.620878\n",
      "step =  25  energy_in =  4.618353  energy_out =  4.549947\n",
      "step =  26  energy_in =  4.547145  energy_out =  4.4978094\n",
      "step =  27  energy_in =  4.4952893  energy_out =  4.433244\n",
      "step =  28  energy_in =  4.429595  energy_out =  4.3916655\n",
      "step =  29  energy_in =  4.3916926  energy_out =  4.3425617\n",
      "step =  30  energy_in =  4.3417115  energy_out =  4.304816\n",
      "step =  31  energy_in =  4.309376  energy_out =  4.281472\n",
      "step =  32  energy_in =  4.2903724  energy_out =  4.2432256\n",
      "step =  33  energy_in =  4.2613287  energy_out =  4.180787\n",
      "step =  34  energy_in =  4.209013  energy_out =  4.1388545\n",
      "step =  35  energy_in =  4.1587534  energy_out =  4.0682364\n",
      "step =  36  energy_in =  4.0831404  energy_out =  4.0271115\n",
      "step =  37  energy_in =  4.0293097  energy_out =  3.9968376\n",
      "step =  38  energy_in =  3.995953  energy_out =  3.9768517\n",
      "step =  39  energy_in =  3.9733312  energy_out =  3.9475324\n",
      "step =  40  energy_in =  3.9445674  energy_out =  3.907721\n",
      "step =  41  energy_in =  3.9065735  energy_out =  3.88464\n",
      "step =  42  energy_in =  3.8847616  energy_out =  3.8514147\n",
      "step =  43  energy_in =  3.8518736  energy_out =  3.837638\n",
      "step =  44  energy_in =  3.8369787  energy_out =  3.8095512\n",
      "step =  45  energy_in =  3.8094628  energy_out =  3.7875009\n",
      "step =  46  energy_in =  3.7876313  energy_out =  3.7668588\n",
      "step =  47  energy_in =  3.7667162  energy_out =  3.7499764\n",
      "step =  48  energy_in =  3.7500587  energy_out =  3.726935\n",
      "step =  49  energy_in =  3.726657  energy_out =  3.7086072\n",
      "step =  50  energy_in =  3.708423  energy_out =  3.6837811\n",
      "step =  51  energy_in =  3.6837974  energy_out =  3.6748466\n",
      "step =  52  energy_in =  3.6748056  energy_out =  3.6542616\n",
      "step =  53  energy_in =  3.6536293  energy_out =  3.6385705\n",
      "step =  54  energy_in =  3.6387956  energy_out =  3.622076\n",
      "step =  55  energy_in =  3.622346  energy_out =  3.61387\n",
      "step =  56  energy_in =  3.6141121  energy_out =  3.5903456\n",
      "step =  57  energy_in =  3.590551  energy_out =  3.5849338\n",
      "step =  58  energy_in =  3.585442  energy_out =  3.5602868\n",
      "step =  59  energy_in =  3.5609539  energy_out =  3.5656989\n",
      "step =  60  energy_in =  3.565582  energy_out =  3.5444508\n",
      "step =  61  energy_in =  3.5457547  energy_out =  3.5502598\n",
      "step =  62  energy_in =  3.5530005  energy_out =  3.542326\n",
      "step =  63  energy_in =  3.545856  energy_out =  3.5576212\n",
      "step =  64  energy_in =  3.5651648  energy_out =  3.5438464\n",
      "step =  65  energy_in =  3.559567  energy_out =  3.5580568\n",
      "step =  66  energy_in =  3.5764894  energy_out =  3.511718\n",
      "step =  67  energy_in =  3.5479167  energy_out =  3.5161035\n",
      "step =  68  energy_in =  3.5384893  energy_out =  3.454303\n",
      "step =  69  energy_in =  3.479097  energy_out =  3.453568\n",
      "step =  70  energy_in =  3.4619648  energy_out =  3.4234667\n",
      "step =  71  energy_in =  3.4263818  energy_out =  3.416009\n",
      "step =  72  energy_in =  3.4159193  energy_out =  3.4044929\n",
      "step =  73  energy_in =  3.4030473  energy_out =  3.393261\n",
      "step =  74  energy_in =  3.3918018  energy_out =  3.3870523\n",
      "step =  75  energy_in =  3.3864095  energy_out =  3.3726757\n",
      "step =  76  energy_in =  3.3727746  energy_out =  3.368829\n",
      "step =  77  energy_in =  3.3693964  energy_out =  3.3550684\n",
      "step =  78  energy_in =  3.3558347  energy_out =  3.3529255\n",
      "step =  79  energy_in =  3.353497  energy_out =  3.3401399\n",
      "step =  80  energy_in =  3.3408504  energy_out =  3.3359041\n",
      "step =  81  energy_in =  3.3364105  energy_out =  3.3252344\n",
      "step =  82  energy_in =  3.3257494  energy_out =  3.3212347\n",
      "step =  83  energy_in =  3.3216627  energy_out =  3.3114557\n",
      "step =  84  energy_in =  3.3118567  energy_out =  3.3075256\n",
      "step =  85  energy_in =  3.307887  energy_out =  3.2972622\n",
      "step =  86  energy_in =  3.297626  energy_out =  3.2941751\n",
      "step =  87  energy_in =  3.2945495  energy_out =  3.2840228\n",
      "step =  88  energy_in =  3.2844005  energy_out =  3.2823858\n",
      "step =  89  energy_in =  3.2827284  energy_out =  3.272533\n",
      "step =  90  energy_in =  3.2729883  energy_out =  3.2709572\n",
      "step =  91  energy_in =  3.271312  energy_out =  3.260164\n",
      "step =  92  energy_in =  3.2608576  energy_out =  3.2597082\n",
      "step =  93  energy_in =  3.260385  energy_out =  3.2489893\n",
      "step =  94  energy_in =  3.2499626  energy_out =  3.2511742\n",
      "step =  95  energy_in =  3.2521756  energy_out =  3.238563\n",
      "step =  96  energy_in =  3.2401843  energy_out =  3.2442265\n",
      "step =  97  energy_in =  3.2457848  energy_out =  3.2310085\n",
      "step =  98  energy_in =  3.2333827  energy_out =  3.2400727\n",
      "step =  99  energy_in =  3.2421787  energy_out =  3.226289\n",
      "step =  100  energy_in =  3.230111  energy_out =  3.2388635\n",
      "step =  101  energy_in =  3.2429042  energy_out =  3.2264931\n",
      "step =  102  energy_in =  3.2333262  energy_out =  3.24485\n",
      "step =  103  energy_in =  3.2503765  energy_out =  3.2261577\n",
      "step =  104  energy_in =  3.2376258  energy_out =  3.2447748\n",
      "step =  105  energy_in =  3.255077  energy_out =  3.218389\n",
      "step =  106  energy_in =  3.2366757  energy_out =  3.236603\n",
      "step =  107  energy_in =  3.2505798  energy_out =  3.2049332\n",
      "step =  108  energy_in =  3.2244718  energy_out =  3.2190795\n",
      "step =  109  energy_in =  3.2312176  energy_out =  3.1868863\n",
      "step =  110  energy_in =  3.198436  energy_out =  3.1938539\n",
      "step =  111  energy_in =  3.201066  energy_out =  3.174208\n",
      "step =  112  energy_in =  3.1785848  energy_out =  3.1765792\n",
      "step =  113  energy_in =  3.1788435  energy_out =  3.1643407\n",
      "step =  114  energy_in =  3.165161  energy_out =  3.1621685\n",
      "step =  115  energy_in =  3.1624193  energy_out =  3.1551714\n",
      "step =  116  energy_in =  3.1550994  energy_out =  3.152114\n",
      "step =  117  energy_in =  3.1519542  energy_out =  3.1480281\n",
      "step =  118  energy_in =  3.1479359  energy_out =  3.1449935\n",
      "step =  119  energy_in =  3.1449134  energy_out =  3.142081\n",
      "step =  120  energy_in =  3.1419647  energy_out =  3.1376095\n",
      "step =  121  energy_in =  3.137392  energy_out =  3.1331549\n",
      "step =  122  energy_in =  3.1330225  energy_out =  3.1280925\n",
      "step =  123  energy_in =  3.1280663  energy_out =  3.1254787\n",
      "step =  124  energy_in =  3.1254306  energy_out =  3.1212263\n",
      "step =  125  energy_in =  3.1211336  energy_out =  3.1183777\n",
      "step =  126  energy_in =  3.1183052  energy_out =  3.1138864\n",
      "step =  127  energy_in =  3.1138773  energy_out =  3.1118696\n",
      "step =  128  energy_in =  3.1118424  energy_out =  3.1081276\n",
      "step =  129  energy_in =  3.108106  energy_out =  3.1062238\n",
      "step =  130  energy_in =  3.1061618  energy_out =  3.101655\n",
      "step =  131  energy_in =  3.1015894  energy_out =  3.0990891\n",
      "step =  132  energy_in =  3.099063  energy_out =  3.0943577\n",
      "step =  133  energy_in =  3.0943177  energy_out =  3.0920813\n",
      "step =  134  energy_in =  3.0920496  energy_out =  3.0868618\n",
      "step =  135  energy_in =  3.0868459  energy_out =  3.0849502\n",
      "step =  136  energy_in =  3.0849125  energy_out =  3.0794184\n",
      "step =  137  energy_in =  3.0794506  energy_out =  3.07853\n",
      "step =  138  energy_in =  3.078527  energy_out =  3.0733213\n",
      "step =  139  energy_in =  3.0733125  energy_out =  3.072843\n",
      "step =  140  energy_in =  3.0727434  energy_out =  3.0673592\n",
      "step =  141  energy_in =  3.0673866  energy_out =  3.0671997\n",
      "step =  142  energy_in =  3.0672014  energy_out =  3.0615134\n",
      "step =  143  energy_in =  3.061563  energy_out =  3.0630467\n",
      "step =  144  energy_in =  3.0630631  energy_out =  3.0567079\n",
      "step =  145  energy_in =  3.0567403  energy_out =  3.0600796\n",
      "step =  146  energy_in =  3.0601125  energy_out =  3.0529869\n",
      "step =  147  energy_in =  3.0531127  energy_out =  3.0600936\n",
      "step =  148  energy_in =  3.0600011  energy_out =  3.0527868\n",
      "step =  149  energy_in =  3.0530446  energy_out =  3.063831\n",
      "step =  150  energy_in =  3.0642822  energy_out =  3.0571194\n",
      "step =  151  energy_in =  3.057883  energy_out =  3.0752978\n",
      "step =  152  energy_in =  3.0760589  energy_out =  3.0710788\n",
      "step =  153  energy_in =  3.07376  energy_out =  3.096345\n",
      "step =  154  energy_in =  3.0998554  energy_out =  3.095788\n",
      "step =  155  energy_in =  3.1075683  energy_out =  3.1243942\n",
      "step =  156  energy_in =  3.1379247  energy_out =  3.0996373\n",
      "step =  157  energy_in =  3.1350353  energy_out =  3.1197762\n",
      "step =  158  energy_in =  3.1429553  energy_out =  3.052888\n",
      "step =  159  energy_in =  3.088026  energy_out =  3.0718458\n",
      "step =  160  energy_in =  3.0834947  energy_out =  3.0350876\n",
      "step =  161  energy_in =  3.0382078  energy_out =  3.039413\n",
      "step =  162  energy_in =  3.0376163  energy_out =  3.033332\n",
      "step =  163  energy_in =  3.029733  energy_out =  3.025667\n",
      "step =  164  energy_in =  3.02359  energy_out =  3.0259583\n",
      "step =  165  energy_in =  3.0260394  energy_out =  3.0145278\n",
      "step =  166  energy_in =  3.0158813  energy_out =  3.0202434\n",
      "step =  167  energy_in =  3.021781  energy_out =  3.0095828\n",
      "step =  168  energy_in =  3.0111265  energy_out =  3.0142753\n",
      "step =  169  energy_in =  3.0153563  energy_out =  3.00581\n",
      "step =  170  energy_in =  3.0063887  energy_out =  3.0073447\n",
      "step =  171  energy_in =  3.0074778  energy_out =  3.0022097\n",
      "step =  172  energy_in =  3.0022352  energy_out =  3.001146\n",
      "step =  173  energy_in =  3.0011206  energy_out =  2.9992263\n",
      "step =  174  energy_in =  2.9991548  energy_out =  2.9977112\n",
      "step =  175  energy_in =  2.9976208  energy_out =  2.9963524\n",
      "step =  176  energy_in =  2.996251  energy_out =  2.9935482\n",
      "step =  177  energy_in =  2.993434  energy_out =  2.9917428\n",
      "step =  178  energy_in =  2.9916677  energy_out =  2.9887066\n",
      "step =  179  energy_in =  2.98868  energy_out =  2.9882412\n",
      "step =  180  energy_in =  2.988237  energy_out =  2.9862823\n",
      "step =  181  energy_in =  2.9862726  energy_out =  2.9867313\n",
      "step =  182  energy_in =  2.9867067  energy_out =  2.984363\n",
      "step =  183  energy_in =  2.9842894  energy_out =  2.983753\n",
      "step =  184  energy_in =  2.983665  energy_out =  2.9801178\n",
      "step =  185  energy_in =  2.9800858  energy_out =  2.9795156\n",
      "step =  186  energy_in =  2.9795156  energy_out =  2.9764895\n",
      "step =  187  energy_in =  2.9764712  energy_out =  2.9762812\n",
      "step =  188  energy_in =  2.9762683  energy_out =  2.9728487\n",
      "step =  189  energy_in =  2.9728386  energy_out =  2.9723692\n",
      "step =  190  energy_in =  2.9723766  energy_out =  2.968585\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4035/460729078.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogressive_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m501\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_plots\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_4035/1856158809.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, steps, K, save_plots, save_fRfO, location)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                     \u001b[0menergy_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menergy_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwatched_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4035/3488540379.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(M, phi_in, fR, fO, PM, initial, direct_training)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirect_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mphi_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmarch_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0menergy_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_energy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphi_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0menergy_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_energy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphi_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4035/2292011941.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(phi)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mintegral\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgrad2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m#riemann sum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0menergy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mhy\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mintegral\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0menergy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/ops/weak_tensor_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_auto_dtype_conversion_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0mbound_arguments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mbound_arguments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0mbound_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbound_arguments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1260\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1261\u001b[0;31m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1262\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m         \u001b[0;31m# TypeError, when given unexpected types.  So we need to catch both.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(input_tensor, axis, keepdims, name)\u001b[0m\n\u001b[1;32m   2428\u001b[0m   \u001b[0mint64\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2429\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mend_compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2430\u001b[0m   \"\"\"\n\u001b[1;32m   2431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2432\u001b[0;31m   return reduce_sum_with_dims(input_tensor, axis, keepdims, name,\n\u001b[0m\u001b[1;32m   2433\u001b[0m                               _ReductionDims(input_tensor, axis))\n",
      "\u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(input_tensor, axis, keepdims, name, dims)\u001b[0m\n\u001b[1;32m   2440\u001b[0m                          dims=None):\n\u001b[1;32m   2441\u001b[0m   \u001b[0mkeepdims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2442\u001b[0m   return _may_reduce_to_scalar(\n\u001b[1;32m   2443\u001b[0m       \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2444\u001b[0;31m       gen_math_ops._sum(input_tensor, dims, keepdims, name=name))\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(input, axis, keep_dims, name)\u001b[0m\n\u001b[1;32m  12300\u001b[0m         _ctx, \"Sum\", name, input, axis, \"keep_dims\", keep_dims)\n\u001b[1;32m  12301\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  12302\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  12303\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 12304\u001b[0;31m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  12305\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  12306\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  12307\u001b[0m       return _sum_eager_fallback(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.progressive_method(steps=501, K=10, save_plots=True, location=location)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "815a983d1180f45e43b0f9e623859d6adc5008144848ba59c486e4fdc876a1b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
